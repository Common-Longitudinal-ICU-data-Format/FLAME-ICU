{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f93815bb",
   "metadata": {
    "papermill": {
     "duration": 0.004817,
     "end_time": "2025-07-31T22:09:36.778411",
     "exception": false,
     "start_time": "2025-07-31T22:09:36.773594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ICU Mortality Model - Dataset Statistics\n",
    "\n",
    "This notebook loads the event-wide dataset and computes comprehensive statistics for the entire dataset.\n",
    "\n",
    "## Objective\n",
    "- Load event-wide dataset from 02_feature_engineering.ipynb\n",
    "- Calculate min, max, mean, median, and missing percentage for all numeric features\n",
    "- Create a single-row summary DataFrame with all statistics\n",
    "- Save results for reference\n",
    "\n",
    "## Statistics Computed\n",
    "- **Min/Max**: Minimum and maximum values for each numeric feature\n",
    "- **Mean/Median**: Central tendency measures\n",
    "- **Missing %**: Percentage of missing values for each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b59337e",
   "metadata": {
    "papermill": {
     "duration": 0.002095,
     "end_time": "2025-07-31T22:09:36.783354",
     "exception": false,
     "start_time": "2025-07-31T22:09:36.781259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccb9b89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T22:09:36.789148Z",
     "iopub.status.busy": "2025-07-31T22:09:36.788832Z",
     "iopub.status.idle": "2025-07-31T22:09:36.970728Z",
     "shell.execute_reply": "2025-07-31T22:09:36.970533Z"
    },
    "papermill": {
     "duration": 0.185753,
     "end_time": "2025-07-31T22:09:36.971363",
     "exception": false,
     "start_time": "2025-07-31T22:09:36.785610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== ICU Mortality Model - Dataset Statistics ===\")\n",
    "print(\"Computing comprehensive statistics for preprocessed datasets...\")\n",
    "\n",
    "# Define paths\n",
    "data_path = os.path.join('..', '..', 'protected_outputs', 'preprocessing')\n",
    "output_path = os.path.join('..', '..', 'protected_outputs', 'preprocessing', 'dataset_statistics.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350c0a6",
   "metadata": {
    "papermill": {
     "duration": 0.000693,
     "end_time": "2025-07-31T22:09:36.972964",
     "exception": false,
     "start_time": "2025-07-31T22:09:36.972271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Event-Wide Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a647f4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T22:09:36.974679Z",
     "iopub.status.busy": "2025-07-31T22:09:36.974572Z",
     "iopub.status.idle": "2025-07-31T22:09:37.040586Z",
     "shell.execute_reply": "2025-07-31T22:09:37.040220Z"
    },
    "papermill": {
     "duration": 0.067505,
     "end_time": "2025-07-31T22:09:37.041135",
     "exception": true,
     "start_time": "2025-07-31T22:09:36.973630",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load event-wide dataset from 02_feature_engineering.ipynb\n",
    "event_wide_path = os.path.join('..', '..', 'protected_outputs', 'preprocessing', 'by_event_wide_df.parquet')\n",
    "\n",
    "if os.path.exists(event_wide_path):\n",
    "    event_wide_df = pd.read_parquet(event_wide_path)\n",
    "    \n",
    "    print(f\"✅ Loaded event-wide dataset: {event_wide_df.shape}\")\n",
    "    print(f\"Hospitalizations: {event_wide_df['hospitalization_id'].nunique()}\")\n",
    "    print(f\"Time range: {event_wide_df['event_time'].min()} to {event_wide_df['event_time'].max()}\")\n",
    "    print(f\"Memory usage: {event_wide_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "else:\n",
    "    raise FileNotFoundError(f\"Event-wide dataset not found at {event_wide_path}. Please run 02_feature_engineering.ipynb first.\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nDataset info:\")\n",
    "print(f\"Total records: {len(event_wide_df):,}\")\n",
    "print(f\"Total columns: {len(event_wide_df.columns)}\")\n",
    "print(f\"Mortality rate: {event_wide_df['disposition'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be296f36",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Identify Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031cbbe2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify numeric columns for statistics calculation\n",
    "# Exclude non-numeric identifier and datetime columns\n",
    "exclude_columns = [\n",
    "    'hospitalization_id', 'event_time', 'hour_24_start_dttm', \n",
    "    'hour_24_end_dttm', 'disposition'\n",
    "]\n",
    "\n",
    "# Get numeric columns\n",
    "numeric_columns = event_wide_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_columns = [col for col in numeric_columns if col not in exclude_columns]\n",
    "\n",
    "print(f\"✅ Identified {len(numeric_columns)} numeric columns for statistics\")\n",
    "print(f\"Total features to analyze: {len(numeric_columns)}\")\n",
    "\n",
    "# Show sample of columns\n",
    "print(\"\\nSample numeric columns:\")\n",
    "for i, col in enumerate(numeric_columns[:10]):\n",
    "    non_null_count = event_wide_df[col].notna().sum()\n",
    "    print(f\"  {col}: {non_null_count:,} non-null values\")\n",
    "if len(numeric_columns) > 10:\n",
    "    print(f\"  ... and {len(numeric_columns) - 10} more columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb5507",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Calculate Comprehensive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1faa69",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate statistics for all numeric columns\n",
    "print(\"Calculating comprehensive statistics for all numeric features...\")\n",
    "\n",
    "# Initialize dictionary to store all statistics\n",
    "stats_dict = {}\n",
    "\n",
    "# Calculate statistics for each numeric column\n",
    "for col in numeric_columns:\n",
    "    try:\n",
    "        # Get the column data\n",
    "        col_data = event_wide_df[col]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats_dict[f\"{col}_min\"] = col_data.min()\n",
    "        stats_dict[f\"{col}_max\"] = col_data.max()\n",
    "        stats_dict[f\"{col}_mean\"] = col_data.mean()\n",
    "        stats_dict[f\"{col}_median\"] = col_data.median()\n",
    "        stats_dict[f\"{col}_missing_pct\"] = (col_data.isna().sum() / len(col_data)) * 100\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate statistics for {col}: {str(e)}\")\n",
    "        # Set NaN values for failed calculations\n",
    "        stats_dict[f\"{col}_min\"] = np.nan\n",
    "        stats_dict[f\"{col}_max\"] = np.nan\n",
    "        stats_dict[f\"{col}_mean\"] = np.nan\n",
    "        stats_dict[f\"{col}_median\"] = np.nan\n",
    "        stats_dict[f\"{col}_missing_pct\"] = np.nan\n",
    "\n",
    "print(f\"✅ Calculated statistics for {len(numeric_columns)} features\")\n",
    "print(f\"Total statistics computed: {len(stats_dict)}\")\n",
    "print(f\"Statistics per feature: 5 (min, max, mean, median, missing%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec85f0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Create Single-Row Summary DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6753b25",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create single-row DataFrame with all statistics\n",
    "print(\"Creating single-row summary DataFrame...\")\n",
    "\n",
    "# Convert statistics dictionary to single-row DataFrame\n",
    "summary_df = pd.DataFrame([stats_dict])\n",
    "\n",
    "# Add metadata columns\n",
    "summary_df['total_records'] = len(event_wide_df)\n",
    "summary_df['total_hospitalizations'] = event_wide_df['hospitalization_id'].nunique()\n",
    "summary_df['total_features_analyzed'] = len(numeric_columns)\n",
    "summary_df['overall_mortality_rate'] = event_wide_df['disposition'].mean()\n",
    "summary_df['analysis_timestamp'] = pd.Timestamp.now()\n",
    "\n",
    "print(f\"✅ Created summary DataFrame: {summary_df.shape}\")\n",
    "print(f\"Total columns in summary: {len(summary_df.columns)}\")\n",
    "\n",
    "# Display sample of statistics\n",
    "print(\"\\nSample statistics (first few features):\")\n",
    "sample_cols = [col for col in summary_df.columns if any(col.endswith(suffix) for suffix in ['_min', '_max', '_mean', '_median', '_missing_pct'])][:15]\n",
    "if sample_cols:\n",
    "    print(summary_df[sample_cols].T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583daea4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Summary Statistics Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151f812",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Provide overview of the statistics\n",
    "print(\"=== Dataset Statistics Overview ===\")\n",
    "\n",
    "# Dataset metadata\n",
    "print(f\"Dataset size: {summary_df['total_records'].iloc[0]:,} records\")\n",
    "print(f\"Hospitalizations: {summary_df['total_hospitalizations'].iloc[0]:,}\")\n",
    "print(f\"Features analyzed: {summary_df['total_features_analyzed'].iloc[0]}\")\n",
    "print(f\"Overall mortality rate: {summary_df['overall_mortality_rate'].iloc[0]:.3f}\")\n",
    "\n",
    "# Statistics summary\n",
    "missing_pct_cols = [col for col in summary_df.columns if col.endswith('_missing_pct')]\n",
    "if missing_pct_cols:\n",
    "    missing_values = summary_df[missing_pct_cols].iloc[0]\n",
    "    print(f\"\\nMissing data overview:\")\n",
    "    print(f\"  Features with no missing data: {(missing_values == 0).sum()}\")\n",
    "    print(f\"  Features with <10% missing: {(missing_values < 10).sum()}\")\n",
    "    print(f\"  Features with 10-50% missing: {((missing_values >= 10) & (missing_values < 50)).sum()}\")\n",
    "    print(f\"  Features with >50% missing: {(missing_values >= 50).sum()}\")\n",
    "    print(f\"  Average missing percentage: {missing_values.mean():.1f}%\")\n",
    "\n",
    "# Show features with highest and lowest missing percentages\n",
    "print(f\"\\nFeatures with lowest missing data:\")\n",
    "lowest_missing = missing_values.nsmallest(5)\n",
    "for feature, pct in lowest_missing.items():\n",
    "    feature_name = feature.replace('_missing_pct', '')\n",
    "    print(f\"  {feature_name}: {pct:.1f}% missing\")\n",
    "\n",
    "print(f\"\\nFeatures with highest missing data:\")\n",
    "highest_missing = missing_values.nlargest(5)\n",
    "for feature, pct in highest_missing.items():\n",
    "    feature_name = feature.replace('_missing_pct', '')\n",
    "    print(f\"  {feature_name}: {pct:.1f}% missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b29e82",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5614e0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save summary statistics to parquet file\n",
    "output_path = os.path.join(output_dir, 'dataset_statistics.parquet')\n",
    "\n",
    "try:\n",
    "    summary_df.to_parquet(output_path, index=False)\n",
    "    print(f\"✅ Saved dataset statistics to: {output_path}\")\n",
    "    \n",
    "    # Verify file was saved\n",
    "    file_size = os.path.getsize(output_path) / 1024  # KB\n",
    "    print(f\"File size: {file_size:.1f} KB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving statistics: {str(e)}\")\n",
    "    # Fallback to CSV if parquet fails\n",
    "    csv_path = os.path.join(output_dir, 'dataset_statistics.csv')\n",
    "    summary_df.to_csv(csv_path, index=False)\n",
    "    print(f\"✅ Saved as CSV instead: {csv_path}\")\n",
    "\n",
    "print(\"\\n=== Analysis Complete ===\") \n",
    "print(f\"Summary DataFrame shape: {summary_df.shape}\")\n",
    "print(f\"Features analyzed: {len(numeric_columns)}\")\n",
    "print(f\"Total statistics generated: {len([col for col in summary_df.columns if any(col.endswith(suffix) for suffix in ['_min', '_max', '_mean', '_median', '_missing_pct'])])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flameICU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 0.955825,
   "end_time": "2025-07-31T22:09:37.156481",
   "environment_variables": {},
   "exception": true,
   "input_path": "03_dataset_statistics.ipynb",
   "output_path": "03_dataset_statistics.ipynb",
   "parameters": {},
   "start_time": "2025-07-31T22:09:36.200656",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
