{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICU Mortality Model - Dataset Statistics\n",
    "\n",
    "This notebook loads the event-wide dataset and computes comprehensive statistics for the entire dataset.\n",
    "\n",
    "## Objective\n",
    "- Load event-wide dataset from 02_feature_engineering.ipynb\n",
    "- Calculate min, max, mean, median, and missing percentage for all numeric features\n",
    "- Create a single-row summary DataFrame with all statistics\n",
    "- Save results for reference\n",
    "\n",
    "## Statistics Computed\n",
    "- **Min/Max**: Minimum and maximum values for each numeric feature\n",
    "- **Mean/Median**: Central tendency measures\n",
    "- **Missing %**: Percentage of missing values for each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure the directory exists\n",
    "output_dir = os.path.join('..', 'output', 'preprocessing')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"=== ICU Mortality Model - Dataset Statistics ===\")\n",
    "print(\"Setting up environment...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Event-Wide Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load event-wide dataset from 02_feature_engineering.ipynb\n",
    "event_wide_path = os.path.join('..', 'output', 'preprocessing', 'by_event_wide_df.parquet')\n",
    "\n",
    "if os.path.exists(event_wide_path):\n",
    "    event_wide_df = pd.read_parquet(event_wide_path)\n",
    "    \n",
    "    print(f\"✅ Loaded event-wide dataset: {event_wide_df.shape}\")\n",
    "    print(f\"Hospitalizations: {event_wide_df['hospitalization_id'].nunique()}\")\n",
    "    print(f\"Time range: {event_wide_df['event_time'].min()} to {event_wide_df['event_time'].max()}\")\n",
    "    print(f\"Memory usage: {event_wide_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "else:\n",
    "    raise FileNotFoundError(f\"Event-wide dataset not found at {event_wide_path}. Please run 02_feature_engineering.ipynb first.\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nDataset info:\")\n",
    "print(f\"Total records: {len(event_wide_df):,}\")\n",
    "print(f\"Total columns: {len(event_wide_df.columns)}\")\n",
    "print(f\"Mortality rate: {event_wide_df['disposition'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns for statistics calculation\n",
    "# Exclude non-numeric identifier and datetime columns\n",
    "exclude_columns = [\n",
    "    'hospitalization_id', 'event_time', 'hour_24_start_dttm', \n",
    "    'hour_24_end_dttm', 'disposition'\n",
    "]\n",
    "\n",
    "# Get numeric columns\n",
    "numeric_columns = event_wide_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_columns = [col for col in numeric_columns if col not in exclude_columns]\n",
    "\n",
    "print(f\"✅ Identified {len(numeric_columns)} numeric columns for statistics\")\n",
    "print(f\"Total features to analyze: {len(numeric_columns)}\")\n",
    "\n",
    "# Show sample of columns\n",
    "print(\"\\nSample numeric columns:\")\n",
    "for i, col in enumerate(numeric_columns[:10]):\n",
    "    non_null_count = event_wide_df[col].notna().sum()\n",
    "    print(f\"  {col}: {non_null_count:,} non-null values\")\n",
    "if len(numeric_columns) > 10:\n",
    "    print(f\"  ... and {len(numeric_columns) - 10} more columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Comprehensive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics for all numeric columns\n",
    "print(\"Calculating comprehensive statistics for all numeric features...\")\n",
    "\n",
    "# Initialize dictionary to store all statistics\n",
    "stats_dict = {}\n",
    "\n",
    "# Calculate statistics for each numeric column\n",
    "for col in numeric_columns:\n",
    "    try:\n",
    "        # Get the column data\n",
    "        col_data = event_wide_df[col]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats_dict[f\"{col}_min\"] = col_data.min()\n",
    "        stats_dict[f\"{col}_max\"] = col_data.max()\n",
    "        stats_dict[f\"{col}_mean\"] = col_data.mean()\n",
    "        stats_dict[f\"{col}_median\"] = col_data.median()\n",
    "        stats_dict[f\"{col}_missing_pct\"] = (col_data.isna().sum() / len(col_data)) * 100\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate statistics for {col}: {str(e)}\")\n",
    "        # Set NaN values for failed calculations\n",
    "        stats_dict[f\"{col}_min\"] = np.nan\n",
    "        stats_dict[f\"{col}_max\"] = np.nan\n",
    "        stats_dict[f\"{col}_mean\"] = np.nan\n",
    "        stats_dict[f\"{col}_median\"] = np.nan\n",
    "        stats_dict[f\"{col}_missing_pct\"] = np.nan\n",
    "\n",
    "print(f\"✅ Calculated statistics for {len(numeric_columns)} features\")\n",
    "print(f\"Total statistics computed: {len(stats_dict)}\")\n",
    "print(f\"Statistics per feature: 5 (min, max, mean, median, missing%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Single-Row Summary DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create single-row DataFrame with all statistics\n",
    "print(\"Creating single-row summary DataFrame...\")\n",
    "\n",
    "# Convert statistics dictionary to single-row DataFrame\n",
    "summary_df = pd.DataFrame([stats_dict])\n",
    "\n",
    "# Add metadata columns\n",
    "summary_df['total_records'] = len(event_wide_df)\n",
    "summary_df['total_hospitalizations'] = event_wide_df['hospitalization_id'].nunique()\n",
    "summary_df['total_features_analyzed'] = len(numeric_columns)\n",
    "summary_df['overall_mortality_rate'] = event_wide_df['disposition'].mean()\n",
    "summary_df['analysis_timestamp'] = pd.Timestamp.now()\n",
    "\n",
    "print(f\"✅ Created summary DataFrame: {summary_df.shape}\")\n",
    "print(f\"Total columns in summary: {len(summary_df.columns)}\")\n",
    "\n",
    "# Display sample of statistics\n",
    "print(\"\\nSample statistics (first few features):\")\n",
    "sample_cols = [col for col in summary_df.columns if any(col.endswith(suffix) for suffix in ['_min', '_max', '_mean', '_median', '_missing_pct'])][:15]\n",
    "if sample_cols:\n",
    "    print(summary_df[sample_cols].T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide overview of the statistics\n",
    "print(\"=== Dataset Statistics Overview ===\")\n",
    "\n",
    "# Dataset metadata\n",
    "print(f\"Dataset size: {summary_df['total_records'].iloc[0]:,} records\")\n",
    "print(f\"Hospitalizations: {summary_df['total_hospitalizations'].iloc[0]:,}\")\n",
    "print(f\"Features analyzed: {summary_df['total_features_analyzed'].iloc[0]}\")\n",
    "print(f\"Overall mortality rate: {summary_df['overall_mortality_rate'].iloc[0]:.3f}\")\n",
    "\n",
    "# Statistics summary\n",
    "missing_pct_cols = [col for col in summary_df.columns if col.endswith('_missing_pct')]\n",
    "if missing_pct_cols:\n",
    "    missing_values = summary_df[missing_pct_cols].iloc[0]\n",
    "    print(f\"\\nMissing data overview:\")\n",
    "    print(f\"  Features with no missing data: {(missing_values == 0).sum()}\")\n",
    "    print(f\"  Features with <10% missing: {(missing_values < 10).sum()}\")\n",
    "    print(f\"  Features with 10-50% missing: {((missing_values >= 10) & (missing_values < 50)).sum()}\")\n",
    "    print(f\"  Features with >50% missing: {(missing_values >= 50).sum()}\")\n",
    "    print(f\"  Average missing percentage: {missing_values.mean():.1f}%\")\n",
    "\n",
    "# Show features with highest and lowest missing percentages\n",
    "print(f\"\\nFeatures with lowest missing data:\")\n",
    "lowest_missing = missing_values.nsmallest(5)\n",
    "for feature, pct in lowest_missing.items():\n",
    "    feature_name = feature.replace('_missing_pct', '')\n",
    "    print(f\"  {feature_name}: {pct:.1f}% missing\")\n",
    "\n",
    "print(f\"\\nFeatures with highest missing data:\")\n",
    "highest_missing = missing_values.nlargest(5)\n",
    "for feature, pct in highest_missing.items():\n",
    "    feature_name = feature.replace('_missing_pct', '')\n",
    "    print(f\"  {feature_name}: {pct:.1f}% missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary statistics to parquet file\n",
    "output_path = os.path.join(output_dir, 'dataset_statistics.parquet')\n",
    "\n",
    "try:\n",
    "    summary_df.to_parquet(output_path, index=False)\n",
    "    print(f\"✅ Saved dataset statistics to: {output_path}\")\n",
    "    \n",
    "    # Verify file was saved\n",
    "    file_size = os.path.getsize(output_path) / 1024  # KB\n",
    "    print(f\"File size: {file_size:.1f} KB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving statistics: {str(e)}\")\n",
    "    # Fallback to CSV if parquet fails\n",
    "    csv_path = os.path.join(output_dir, 'dataset_statistics.csv')\n",
    "    summary_df.to_csv(csv_path, index=False)\n",
    "    print(f\"✅ Saved as CSV instead: {csv_path}\")\n",
    "\n",
    "print(\"\\n=== Analysis Complete ===\") \n",
    "print(f\"Summary DataFrame shape: {summary_df.shape}\")\n",
    "print(f\"Features analyzed: {len(numeric_columns)}\")\n",
    "print(f\"Total statistics generated: {len([col for col in summary_df.columns if any(col.endswith(suffix) for suffix in ['_min', '_max', '_mean', '_median', '_missing_pct'])])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}