{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d5d68fd",
   "metadata": {
    "papermill": {
     "duration": 0.004707,
     "end_time": "2025-07-31T22:28:53.933760",
     "exception": false,
     "start_time": "2025-07-31T22:28:53.929053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ICU Mortality Model - Feature Engineering\n",
    "\n",
    "This notebook loads the ICU cohort and creates hourly wide dataset for the first 24 hours of ICU stay.\n",
    "\n",
    "## Objective\n",
    "- Load ICU cohort from 01_cohort.ipynb\n",
    "- Use pyCLIF to extract features from CLIF tables\n",
    "- Create hourly wide dataset for the first 24 hours\n",
    "- Filter to encounters with complete 24-hour data\n",
    "- Save features for modeling\n",
    "\n",
    "## Feature Sources\n",
    "- **Vitals**: All vital_category values\n",
    "- **Labs**: All lab_category values\n",
    "- **Patient Assessments**: GCS_total, RASS\n",
    "- **Respiratory Support**: Mode, FiO2, PEEP, ventilator settings (with one-hot encoding)\n",
    "- **Medications**: All vasoactives and sedatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a4029",
   "metadata": {
    "papermill": {
     "duration": 0.002834,
     "end_time": "2025-07-31T22:28:53.940322",
     "exception": false,
     "start_time": "2025-07-31T22:28:53.937488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "**Memory Management Notes:**\n",
    "- This notebook processes a large dataset (54K+ hospitalizations) \n",
    "- If you encounter kernel crashes or memory errors:\n",
    "  1. Set `USE_SAMPLE_DATA=True` in the configuration cell below\n",
    "  2. Increase `memory_limit` parameter in the hourly aggregation function\n",
    "  3. Reduce `batch_size` parameters if needed\n",
    "- The hourly aggregation function uses DuckDB for optimal performance and automatically handles batching for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f966c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T22:28:53.948204Z",
     "iopub.status.busy": "2025-07-31T22:28:53.947961Z",
     "iopub.status.idle": "2025-07-31T22:28:54.155125Z",
     "shell.execute_reply": "2025-07-31T22:28:54.154917Z"
    },
    "papermill": {
     "duration": 0.212273,
     "end_time": "2025-07-31T22:28:54.155767",
     "exception": false,
     "start_time": "2025-07-31T22:28:53.943494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ICU Mortality Model - Feature Engineering ===\n",
      "Setting up environment...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyclif import CLIF\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Ensure the directory exists\n",
    "output_dir = os.path.join('..', '..', 'protected_outputs', 'preprocessing')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(\"=== ICU Mortality Model - Feature Engineering ===\")\n",
    "print(\"Setting up environment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f39772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T22:28:54.158071Z",
     "iopub.status.busy": "2025-07-31T22:28:54.157952Z",
     "iopub.status.idle": "2025-07-31T22:28:54.160262Z",
     "shell.execute_reply": "2025-07-31T22:28:54.160093Z"
    },
    "papermill": {
     "duration": 0.003991,
     "end_time": "2025-07-31T22:28:54.160806",
     "exception": false,
     "start_time": "2025-07-31T22:28:54.156815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded configuration from config_demo.json\n",
      "Site: MIMIC\n",
      "Data path: /Users/sudo_sage/Documents/WORK/clif_mimic\n",
      "File type: parquet\n"
     ]
    }
   ],
   "source": [
    "def load_config():\n",
    "    \"\"\"Load configuration from config.json or config_demo.json\"\"\"\n",
    "    # Try top-level config_demo.json first (new location)\n",
    "    config_path = os.path.join(\"..\", \"..\", \"config_demo.json\")\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        # Try config.json in same location\n",
    "        config_path = os.path.join(\"..\", \"..\", \"config.json\")\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        # Fallback to local config_demo.json\n",
    "        config_path = \"config_demo.json\"\n",
    "    \n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = json.load(file)\n",
    "        print(f\"✅ Loaded configuration from {os.path.basename(config_path)}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Configuration file not found. Please create config.json or config_demo.json based on the config_template.\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "print(f\"Site: {config['site']}\")\n",
    "print(f\"Data path: {config['clif2_path']}\")\n",
    "print(f\"File type: {config['filetype']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78438ae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T22:28:54.162796Z",
     "iopub.status.busy": "2025-07-31T22:28:54.162721Z",
     "iopub.status.idle": "2025-07-31T22:28:54.164102Z",
     "shell.execute_reply": "2025-07-31T22:28:54.163927Z"
    },
    "papermill": {
     "duration": 0.002909,
     "end_time": "2025-07-31T22:28:54.164594",
     "exception": false,
     "start_time": "2025-07-31T22:28:54.161685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIF Object Initialized.\n",
      "✅ pyCLIF initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize pyCLIF\n",
    "clif = CLIF(\n",
    "    data_dir=config['clif2_path'],\n",
    "    filetype=config['filetype'],\n",
    "    timezone=\"US/Eastern\"\n",
    ")\n",
    "\n",
    "print(\"✅ pyCLIF initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ff88e",
   "metadata": {
    "papermill": {
     "duration": 0.000757,
     "end_time": "2025-07-31T22:28:54.166152",
     "exception": false,
     "start_time": "2025-07-31T22:28:54.165395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load ICU Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee5e2ea9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T22:28:54.168561Z",
     "iopub.status.busy": "2025-07-31T22:28:54.168448Z",
     "iopub.status.idle": "2025-07-31T22:28:54.216998Z",
     "shell.execute_reply": "2025-07-31T22:28:54.216810Z"
    },
    "papermill": {
     "duration": 0.050403,
     "end_time": "2025-07-31T22:28:54.217531",
     "exception": false,
     "start_time": "2025-07-31T22:28:54.167128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cohort loaded successfully: 54509 hospitalizations\n",
      "Mortality rate: 0.110\n",
      "Time range: 2105-10-04 22:27:12+00:00 to 2214-05-03 22:09:18+00:00\n",
      "\n",
      "Sample cohort records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hospitalization_id</th>\n",
       "      <th>start_dttm</th>\n",
       "      <th>hour_24_start_dttm</th>\n",
       "      <th>hour_24_end_dttm</th>\n",
       "      <th>disposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25860671</td>\n",
       "      <td>2150-11-03 00:37:00+00:00</td>\n",
       "      <td>2150-11-03 00:37:00+00:00</td>\n",
       "      <td>2150-11-04 00:37:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24597018</td>\n",
       "      <td>2157-11-21 00:18:02+00:00</td>\n",
       "      <td>2157-11-21 00:18:02+00:00</td>\n",
       "      <td>2157-11-22 00:18:02+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25563031</td>\n",
       "      <td>2110-04-11 20:52:22+00:00</td>\n",
       "      <td>2110-04-11 20:52:22+00:00</td>\n",
       "      <td>2110-04-12 20:52:22+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23581541</td>\n",
       "      <td>2160-05-18 15:00:53+00:00</td>\n",
       "      <td>2160-05-18 15:00:53+00:00</td>\n",
       "      <td>2160-05-19 15:00:53+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27793700</td>\n",
       "      <td>2162-02-18 04:30:00+00:00</td>\n",
       "      <td>2162-02-18 04:30:00+00:00</td>\n",
       "      <td>2162-02-19 04:30:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hospitalization_id                start_dttm        hour_24_start_dttm  \\\n",
       "0           25860671 2150-11-03 00:37:00+00:00 2150-11-03 00:37:00+00:00   \n",
       "1           24597018 2157-11-21 00:18:02+00:00 2157-11-21 00:18:02+00:00   \n",
       "2           25563031 2110-04-11 20:52:22+00:00 2110-04-11 20:52:22+00:00   \n",
       "3           23581541 2160-05-18 15:00:53+00:00 2160-05-18 15:00:53+00:00   \n",
       "4           27793700 2162-02-18 04:30:00+00:00 2162-02-18 04:30:00+00:00   \n",
       "\n",
       "           hour_24_end_dttm  disposition  \n",
       "0 2150-11-04 00:37:00+00:00            0  \n",
       "1 2157-11-22 00:18:02+00:00            0  \n",
       "2 2110-04-12 20:52:22+00:00            0  \n",
       "3 2160-05-19 15:00:53+00:00            0  \n",
       "4 2162-02-19 04:30:00+00:00            0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ICU cohort from 01_cohort.ipynb\n",
    "cohort_path = os.path.join('..', '..', 'protected_outputs', 'preprocessing', 'icu_cohort.parquet')\n",
    "\n",
    "if os.path.exists(cohort_path):\n",
    "    cohort_df = pd.read_parquet(cohort_path)\n",
    "    print(f\"✅ Cohort loaded successfully: {len(cohort_df)} hospitalizations\")\n",
    "    print(f\"Mortality rate: {cohort_df['disposition'].mean():.3f}\")\n",
    "    \n",
    "    # Convert datetime columns\n",
    "    datetime_cols = ['start_dttm', 'hour_24_start_dttm', 'hour_24_end_dttm']\n",
    "    for col in datetime_cols:\n",
    "        cohort_df[col] = pd.to_datetime(cohort_df[col])\n",
    "    \n",
    "    print(f\"Time range: {cohort_df['start_dttm'].min()} to {cohort_df['start_dttm'].max()}\")\n",
    "    \n",
    "else:\n",
    "    raise FileNotFoundError(f\"Cohort file not found at {cohort_path}. Please run 01_cohort.ipynb first.\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample cohort records:\")\n",
    "cohort_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f196f67c",
   "metadata": {
    "papermill": {
     "duration": 0.000906,
     "end_time": "2025-07-31T22:28:54.219421",
     "exception": false,
     "start_time": "2025-07-31T22:28:54.218515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Extraction Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd45f3f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T22:28:54.221493Z",
     "iopub.status.busy": "2025-07-31T22:28:54.221387Z",
     "iopub.status.idle": "2025-07-31T22:28:54.226224Z",
     "shell.execute_reply": "2025-07-31T22:28:54.226070Z"
    },
    "papermill": {
     "duration": 0.006509,
     "end_time": "2025-07-31T22:28:54.226770",
     "exception": false,
     "start_time": "2025-07-31T22:28:54.220261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring feature extraction...\n",
      "Using full dataset: 54509 hospitalizations\n",
      "\n",
      "Feature extraction configuration:\n",
      "  vitals: 7 categories\n",
      "  labs: 52 categories\n",
      "  medication_admin_continuous: 19 categories\n",
      "  respiratory_support: 3 categories\n",
      "\n",
      "Extracting features for 54509 hospitalizations\n"
     ]
    }
   ],
   "source": [
    "# Define feature extraction configuration\n",
    "print(\"Configuring feature extraction...\")\n",
    "\n",
    "# OPTION: Set to True for development/testing with smaller dataset\n",
    "USE_SAMPLE_DATA = False  # Set to True to use sample for faster processing\n",
    "SAMPLE_SIZE = 1000  # Number of hospitalizations to sample\n",
    "\n",
    "# Get hospitalization IDs from cohort\n",
    "if USE_SAMPLE_DATA:\n",
    "    print(f\"⚠️ Using sample data with {SAMPLE_SIZE} hospitalizations for testing\")\n",
    "    cohort_sample = cohort_df.sample(n=min(SAMPLE_SIZE, len(cohort_df)), random_state=42)\n",
    "    cohort_ids = cohort_sample['hospitalization_id'].astype(str).unique().tolist()\n",
    "    print(f\"Sampled {len(cohort_ids)} hospitalizations from {len(cohort_df)} total\")\n",
    "else:\n",
    "    cohort_ids = cohort_df['hospitalization_id'].astype(str).unique().tolist()\n",
    "    print(f\"Using full dataset: {len(cohort_ids)} hospitalizations\")\n",
    "\n",
    "# Define category filters for each table\n",
    "category_filters = {\n",
    "    'vitals': [  # Common vital signs\n",
    "        'heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c',\n",
    "        'weight_kg', 'height_cm'\n",
    "    ],\n",
    "    'labs': [  # Common lab values\n",
    "        \"albumin\", \"alkaline_phosphatase\", \"alt\", \"ast\", \"basophils_percent\", \"basophils_absolute\", \n",
    "        \"bicarbonate\", \"bilirubin_total\", \"bilirubin_conjugated\", \"bilirubin_unconjugated\",\n",
    "        \"bun\", \"calcium_total\", \"calcium_ionized\", \"chloride\", \"creatinine\", \"crp\", \n",
    "        \"eosinophils_percent\", \"eosinophils_absolute\", \"esr\", \"ferritin\", \"glucose_f≠ingerstick\", \n",
    "        \"glucose_serum\", \"hemoglobin\", \"phosphate\", \"inr\", \"lactate\", \"ldh\",\n",
    "        \"lymphocytes_percent\", \"lymphocytes_absolute\", \"magnesium\", \"monocytes_percent\", \n",
    "        \"monocytes_absolute\", \"neutrophils_percent\", \"neutrophils_absolute\",\n",
    "        \"pco2_arterial\", \"po2_arterial\", \"pco2_venous\", \"ph_arterial\", \"ph_venous\", \n",
    "        \"platelet_count\", \"potassium\", \"procalcitonin\", \"pt\", \"ptt\", \n",
    "        \"so2_arterial\", \"so2_mixed_venous\", \"so2_central_venous\", \"sodium\",\n",
    "        \"total_protein\", \"troponin_i\", \"troponin_t\", \"wbc\"\n",
    "    ],\n",
    "    'medication_admin_continuous': [  # Vasoactives and sedatives\n",
    "        \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"angiotensin\", \"vasopressin\",\n",
    "        \"dopamine\", \"dobutamine\", \"milrinone\", \"isoproterenol\",\n",
    "        \"propofol\", \"dexmedetomidine\", \"ketamine\", \"midazolam\", \"fentanyl\",\n",
    "        \"hydromorphone\", \"morphine\", \"remifentanil\", \"pentobarbital\", \"lorazepam\"\n",
    "    ],\n",
    "    'respiratory_support': [  # All respiratory support categories\n",
    "        'mode_category', 'device_category', 'fio2'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nFeature extraction configuration:\")\n",
    "for table, categories in category_filters.items():\n",
    "    print(f\"  {table}: {len(categories)} categories\")\n",
    "\n",
    "print(f\"\\nExtracting features for {len(cohort_ids)} hospitalizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7edbae",
   "metadata": {
    "papermill": {
     "duration": 0.000847,
     "end_time": "2025-07-31T22:28:54.228559",
     "exception": false,
     "start_time": "2025-07-31T22:28:54.227712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Wide Dataset Using pyCLIF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a938aa",
   "metadata": {
    "papermill": {
     "duration": 0.000841,
     "end_time": "2025-07-31T22:28:54.230205",
     "exception": false,
     "start_time": "2025-07-31T22:28:54.229364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Performance Optimization with Cohort Time Filtering\n",
    "\n",
    "The `create_wide_dataset` function now supports an optional `cohort_df` parameter that allows filtering data to specific time windows **before** creating the wide dataset. This significantly improves performance and reduces memory usage when you only need data from specific time periods.\n",
    "\n",
    "**Benefits:**\n",
    "- Reduces data volume before pivoting operations\n",
    "- Significantly lower memory usage\n",
    "- Faster processing time\n",
    "- Particularly useful for ICU mortality models where we only need the first 24 hours\n",
    "\n",
    "**Required columns in cohort_df:**\n",
    "- `hospitalization_id`: Unique identifier for each hospitalization\n",
    "- `start_time`: Start of the time window (datetime)\n",
    "- `end_time`: End of the time window (datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d13bb288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T22:28:54.232180Z",
     "iopub.status.busy": "2025-07-31T22:28:54.232107Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-07-31T22:28:54.231032",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating wide dataset using pyCLIF...\n",
      "Using cohort_df time filtering for 54509 hospitalizations\n",
      "This will filter data to 24-hour windows before creating the wide dataset\n",
      "Auto-loading required base table: patient\n",
      "Loading clif_patient.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from clif_patient.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed with 8 error(s). See `errors` attribute.\n",
      "Auto-loading required base table: hospitalization\n",
      "Loading clif_hospitalization.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from clif_hospitalization.parquet\n",
      "Validation completed with 1 error(s). See `errors` attribute.\n",
      "Auto-loading required base table: adt\n",
      "Loading clif_adt.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from clif_adt.parquet\n",
      "Validation completed with 3 error(s). See `errors` attribute.\n",
      "Auto-loading table: vitals\n",
      "Loading clif_vitals.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4716f359e8a9439ebdd5d633daa2004d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from clif_vitals.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed with 9 error(s).\n",
      "  - 9 range validation error(s)\n",
      "See `errors` and `range_validation_errors` attributes for details.\n",
      "Auto-loading table: medication_admin_continuous\n",
      "Loading clif_medication_admin_continuous.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from clif_medication_admin_continuous.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed with 4 error(s). See `errors` attribute.\n",
      "Auto-loading table: labs\n",
      "Loading clif_labs.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df051110fd64863be0c9a8cf895f69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create wide dataset for cohort hospitalizations\n",
    "print(\"Creating wide dataset using pyCLIF...\")\n",
    "\n",
    "# Prepare cohort_df with required columns for time filtering\n",
    "# This will significantly reduce memory usage by filtering data to only the 24-hour windows\n",
    "cohort_time_filter = cohort_df[['hospitalization_id', 'hour_24_start_dttm', 'hour_24_end_dttm']].copy()\n",
    "cohort_time_filter.columns = ['hospitalization_id', 'start_time', 'end_time']  # Rename to match expected columns\n",
    "\n",
    "print(f\"Using cohort_df time filtering for {len(cohort_time_filter)} hospitalizations\")\n",
    "print(f\"This will filter data to 24-hour windows before creating the wide dataset\")\n",
    "\n",
    "wide_df = clif.create_wide_dataset(\n",
    "    hospitalization_ids=cohort_ids,\n",
    "    cohort_df=cohort_time_filter,  # Pass cohort_df for time window filtering\n",
    "    category_filters=category_filters,  \n",
    "    save_to_data_location=False,\n",
    "    batch_size=10000,\n",
    "    memory_limit='6GB',\n",
    "    threads=4,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Wide dataset created successfully\")\n",
    "print(f\"Shape: {wide_df.shape}\")\n",
    "print(f\"Hospitalizations: {wide_df['hospitalization_id'].nunique()}\")\n",
    "print(f\"Date range: {wide_df['event_time'].min()} to {wide_df['event_time'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c166571",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wide_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5521cf1a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wide_df[[ 'angiotensin', 'dexmedetomidine',\n",
    "       'dobutamine', 'dopamine', 'epinephrine', 'fentanyl', 'hydromorphone',\n",
    "       'ketamine', 'lorazepam', 'midazolam', 'milrinone', 'morphine',\n",
    "       'norepinephrine', 'pentobarbital', 'phenylephrine', 'propofol',\n",
    "       'vasopressin']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80767da",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Safely inspect a subset of the data to avoid memory issues\n",
    "print(\"Inspecting data sample...\")\n",
    "\n",
    "# Check dataset size first\n",
    "print(f\"Wide dataset shape: {wide_df.shape}\")\n",
    "print(f\"Memory usage: {wide_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Sample a specific hospitalization safely\n",
    "sample_hosp_id = wide_df['hospitalization_id'].iloc[0]\n",
    "print(f\"Examining data for hospitalization: {sample_hosp_id}\")\n",
    "\n",
    "try:\n",
    "    # Use query method which is more memory efficient for large datasets\n",
    "    temp = wide_df.query(f\"hospitalization_id == '{sample_hosp_id}'\")\n",
    "    print(f\"Records for {sample_hosp_id}: {len(temp)}\")\n",
    "    \n",
    "    if len(temp) > 0:\n",
    "        print(\"Time range:\", temp['event_time'].min(), \"to\", temp['event_time'].max())\n",
    "        print(\"Columns with data:\", (temp.notna().sum() > 0).sum())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error inspecting data: {str(e)}\")\n",
    "    print(\"Dataset might be too large for this operation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b58568f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define aggregation configuration - FIXED to match available columns\n",
    "print(\"Defining aggregation configuration...\")\n",
    "\n",
    "# Build aggregation config based on what we actually have\n",
    "aggregation_config = {\n",
    "    # Apply multiple aggregations to vital signs and labs that are actually present\n",
    "    \"max\": [col for col in category_filters['vitals'] + category_filters['labs'] if col in wide_df.columns],\n",
    "    \"min\": [col for col in category_filters['vitals'] + category_filters['labs'] if col in wide_df.columns],\n",
    "   # \"mean\": [col for col in category_filters['vitals'] + category_filters['labs'] if col in wide_df.columns],\n",
    "    \"median\": [col for col in category_filters['vitals'] + category_filters['labs'] if col in wide_df.columns],\n",
    "    # Boolean aggregation for medications (1 if present in hour, 0 otherwise)\n",
    "    \"boolean\": [col for col in category_filters['medication_admin_continuous'] if col in wide_df.columns],\n",
    "    # One-hot encode categorical respiratory support columns\n",
    "    \"one_hot_encode\": [col for col in [\"mode_category\", \"device_category\"] if col in wide_df.columns]\n",
    "}\n",
    "\n",
    "# Print what will actually be aggregated\n",
    "print(\"Aggregation configuration:\")\n",
    "for method, cols in aggregation_config.items():\n",
    "    print(f\"  {method}: {len(cols)} columns\")\n",
    "    if len(cols) <= 10:\n",
    "        print(f\"    {cols}\")\n",
    "    else:\n",
    "        print(f\"    {cols[:5]}...{cols[-2:]} (showing first 5 and last 2)\")\n",
    "\n",
    "# Convert to hourly using optimized DuckDB function\n",
    "print(f\"\\nProcessing {len(wide_df):,} records to hourly aggregation...\")\n",
    "\n",
    "hourly_df = convert_wide_to_hourly(\n",
    "    wide_df, \n",
    "    aggregation_config, \n",
    "    memory_limit='6GB',      # Set memory limit for DuckDB\n",
    "    batch_size=10000          # Process in batches for large datasets\n",
    ")\n",
    "\n",
    "print(\"✅ Hourly aggregation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc64193",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance and Results Summary\n",
    "print(\"=== Hourly Aggregation Results ===\")\n",
    "print(f\"✅ Processing complete!\")\n",
    "print(f\"Input wide dataset: {wide_df.shape[0]:,} records\")\n",
    "print(f\"Output hourly dataset: {hourly_df.shape[0]:,} records\") \n",
    "print(f\"Columns in hourly dataset: {hourly_df.shape[1]}\")\n",
    "print(f\"Compression ratio: {wide_df.shape[0] / hourly_df.shape[0]:.1f}x fewer records\")\n",
    "\n",
    "# Show hourly distribution\n",
    "hourly_stats = hourly_df.groupby('nth_hour').size()\n",
    "print(f\"\\nHourly record distribution:\")\n",
    "print(f\"  Hours covered: 0 to {hourly_stats.index.max()}\")\n",
    "print(f\"  Average records per hour: {hourly_stats.mean():.0f}\")\n",
    "print(f\"  Records in first 24 hours: {hourly_stats[hourly_stats.index < 24].sum():,}\")\n",
    "\n",
    "# Show sample of output columns\n",
    "print(f\"\\nSample of aggregated columns:\")\n",
    "agg_columns = [col for col in hourly_df.columns if any(col.endswith(suffix) for suffix in ['_max', '_min', '_mean', '_boolean'])]\n",
    "for col in agg_columns[:10]:\n",
    "    non_null_count = hourly_df[col].notna().sum()\n",
    "    print(f\"  {col}: {non_null_count:,} non-null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6eb88a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: This filtering step is now redundant if cohort_df was used in create_wide_dataset\n",
    "# The data is already filtered to the 24-hour windows during the wide dataset creation\n",
    "# However, we'll keep this for backward compatibility and verification\n",
    "\n",
    "# Filter wide dataset to 24-hour windows\n",
    "print(\"Filtering to 24-hour windows for event wide data...: Shape:\", wide_df.shape)\n",
    "cohort_df['hospitalization_id'] = cohort_df['hospitalization_id'].astype(str)\n",
    "# Merge with cohort to get time windows\n",
    "wide_df_filtered = pd.merge(\n",
    "    wide_df,\n",
    "    cohort_df[['hospitalization_id', 'hour_24_start_dttm', 'hour_24_end_dttm', 'disposition']],\n",
    "    on='hospitalization_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"After merge with cohort: {len(wide_df_filtered)} records\")\n",
    "\n",
    "print(f\"✅ Filtered to 24-hour windows: {len(wide_df_filtered)} records\")\n",
    "print(f\"Hospitalizations with data: {wide_df_filtered['hospitalization_id'].nunique()}\")\n",
    "\n",
    "# Show time window validation\n",
    "print(\"\\nTime window validation:\")\n",
    "print(f\"All events within window: {((wide_df_filtered['event_time'] >= wide_df_filtered['hour_24_start_dttm']) & (wide_df_filtered['event_time'] <= wide_df_filtered['hour_24_end_dttm'])).all()}\")\n",
    "print(f\"Average records per hospitalization: {len(wide_df_filtered) / wide_df_filtered['hospitalization_id'].nunique():.1f}\")\n",
    "print('Shape: after filtering:', wide_df_filtered.shape)\n",
    "\n",
    "wide_df_filtered.to_parquet(os.path.join(output_dir, 'by_event_wide_df.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c94ea",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter hourly dataset to 24-hour windows\n",
    "print(\"\\nFiltering hourly dataset to 24-hour windows...| Shape:\",hourly_df.shape)\n",
    "# Merge with cohort to get time windows\n",
    "hourly_df_filtered = pd.merge(\n",
    "    hourly_df,\n",
    "    cohort_df[['hospitalization_id', 'hour_24_start_dttm', 'hour_24_end_dttm', 'disposition']],\n",
    "    on='hospitalization_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"After merge with cohort: {len(hourly_df_filtered)} records\")\n",
    "\n",
    "print(f\"✅ Filtered hourly dataset to 24-hour windows: {len(hourly_df_filtered)} records\")\n",
    "print(f\"Hospitalizations with data in hourly dataset: {hourly_df_filtered['hospitalization_id'].nunique()}\")\n",
    "\n",
    "# Show time window validation for hourly dataset\n",
    "print(\"\\nTime window validation for hourly dataset:\")\n",
    "print(f\"All events within window: {((hourly_df_filtered['event_time_hour'] >= hourly_df_filtered['hour_24_start_dttm']) & (hourly_df_filtered['event_time_hour'] <= hourly_df_filtered['hour_24_end_dttm'])).all()}\")\n",
    "print(f\"Average records per hospitalization: {len(hourly_df_filtered) / hourly_df_filtered['hospitalization_id'].nunique():.1f}\")\n",
    "\n",
    "print('Shape:', hourly_df_filtered.shape)\n",
    "hourly_df_filtered.to_parquet(os.path.join(output_dir, 'by_hourly_wide_df.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1457711",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hourly_df_filtered.columns.tolist()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flameICU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "02_feature_engineering.ipynb",
   "output_path": "02_feature_engineering.ipynb",
   "parameters": {},
   "start_time": "2025-07-31T22:28:53.353745",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}