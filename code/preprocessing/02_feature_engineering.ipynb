{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICU Mortality Model - Feature Engineering\n",
    "\n",
    "This notebook loads the ICU cohort and creates hourly wide dataset for the first 24 hours of ICU stay.\n",
    "\n",
    "## Objective\n",
    "- Load ICU cohort from 01_cohort.ipynb\n",
    "- Use pyCLIF to extract features from CLIF tables\n",
    "- Create hourly wide dataset for the first 24 hours\n",
    "- Filter to encounters with complete 24-hour data\n",
    "- Save features for modeling\n",
    "\n",
    "## Feature Sources\n",
    "- **Vitals**: All vital_category values\n",
    "- **Labs**: All lab_category values\n",
    "- **Patient Assessments**: GCS_total, RASS\n",
    "- **Respiratory Support**: Mode, FiO2, PEEP, ventilator settings (with one-hot encoding)\n",
    "- **Medications**: All vasoactives and sedatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "**Memory Management Notes:**\n",
    "- This notebook processes a large dataset (54K+ hospitalizations) \n",
    "- If you encounter kernel crashes or memory errors:\n",
    "  1. Set `USE_SAMPLE_DATA=True` in the configuration cell below\n",
    "  2. Increase `memory_limit` parameter in the hourly aggregation function\n",
    "  3. Reduce `batch_size` parameters if needed\n",
    "- The hourly aggregation function uses DuckDB for optimal performance and automatically handles batching for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ICU Mortality Model - Feature Engineering ===\n",
      "Setting up environment...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyclif import CLIF\n",
    "from pyclif.utils.wide_dataset import convert_wide_to_hourly\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Ensure the directory exists\n",
    "output_dir = os.path.join('..', 'output', 'preprocessing')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(\"=== ICU Mortality Model - Feature Engineering ===\")\n",
    "print(\"Setting up environment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded configuration from config_demo.json\n",
      "Site: MIMIC\n",
      "Data path: /Users/sudo_sage/Documents/WORK/clif_mimic\n",
      "File type: parquet\n"
     ]
    }
   ],
   "source": [
    "def load_config():\n",
    "    \"\"\"Load configuration from config.json or config_demo.json\"\"\"\n",
    "    # Try config.json first, then config_demo.json\n",
    "    config_path = os.path.join(\"config.json\")\n",
    "    if not os.path.exists(config_path):\n",
    "        config_path = os.path.join(\"config_demo.json\")\n",
    "    \n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = json.load(file)\n",
    "        print(f\"✅ Loaded configuration from {os.path.basename(config_path)}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Configuration file not found. Please create config.json or config_demo.json based on the config_template.\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "print(f\"Site: {config['site']}\")\n",
    "print(f\"Data path: {config['clif2_path']}\")\n",
    "print(f\"File type: {config['filetype']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIF Object Initialized.\n",
      "✅ pyCLIF initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize pyCLIF\n",
    "clif = CLIF(\n",
    "    data_dir=config['clif2_path'],\n",
    "    filetype=config['filetype'],\n",
    "    timezone=\"US/Eastern\"\n",
    ")\n",
    "\n",
    "print(\"✅ pyCLIF initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ICU Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded ICU cohort: 54509 hospitalizations\n",
      "Mortality rate: 0.110\n",
      "Time range: 2105-10-04 22:27:12+00:00 to 2214-05-03 22:09:18+00:00\n",
      "\n",
      "Sample cohort records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hospitalization_id</th>\n",
       "      <th>start_dttm</th>\n",
       "      <th>hour_24_start_dttm</th>\n",
       "      <th>hour_24_end_dttm</th>\n",
       "      <th>disposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25860671</td>\n",
       "      <td>2150-11-03 00:37:00+00:00</td>\n",
       "      <td>2150-11-03 00:37:00+00:00</td>\n",
       "      <td>2150-11-04 00:37:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24597018</td>\n",
       "      <td>2157-11-21 00:18:02+00:00</td>\n",
       "      <td>2157-11-21 00:18:02+00:00</td>\n",
       "      <td>2157-11-22 00:18:02+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25563031</td>\n",
       "      <td>2110-04-11 20:52:22+00:00</td>\n",
       "      <td>2110-04-11 20:52:22+00:00</td>\n",
       "      <td>2110-04-12 20:52:22+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23581541</td>\n",
       "      <td>2160-05-18 15:00:53+00:00</td>\n",
       "      <td>2160-05-18 15:00:53+00:00</td>\n",
       "      <td>2160-05-19 15:00:53+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27793700</td>\n",
       "      <td>2162-02-18 04:30:00+00:00</td>\n",
       "      <td>2162-02-18 04:30:00+00:00</td>\n",
       "      <td>2162-02-19 04:30:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hospitalization_id                start_dttm        hour_24_start_dttm  \\\n",
       "0           25860671 2150-11-03 00:37:00+00:00 2150-11-03 00:37:00+00:00   \n",
       "1           24597018 2157-11-21 00:18:02+00:00 2157-11-21 00:18:02+00:00   \n",
       "2           25563031 2110-04-11 20:52:22+00:00 2110-04-11 20:52:22+00:00   \n",
       "3           23581541 2160-05-18 15:00:53+00:00 2160-05-18 15:00:53+00:00   \n",
       "4           27793700 2162-02-18 04:30:00+00:00 2162-02-18 04:30:00+00:00   \n",
       "\n",
       "           hour_24_end_dttm  disposition  \n",
       "0 2150-11-04 00:37:00+00:00            0  \n",
       "1 2157-11-22 00:18:02+00:00            0  \n",
       "2 2110-04-12 20:52:22+00:00            0  \n",
       "3 2160-05-19 15:00:53+00:00            0  \n",
       "4 2162-02-19 04:30:00+00:00            0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ICU cohort from 01_cohort.ipynb\n",
    "cohort_path = os.path.join('..', 'output', 'preprocessing', 'icu_cohort.parquet')\n",
    "\n",
    "if os.path.exists(cohort_path):\n",
    "    cohort_df = pd.read_parquet(cohort_path)\n",
    "    \n",
    "    # Convert datetime columns\n",
    "    datetime_cols = ['start_dttm', 'hour_24_start_dttm', 'hour_24_end_dttm']\n",
    "    for col in datetime_cols:\n",
    "        cohort_df[col] = pd.to_datetime(cohort_df[col])\n",
    "    \n",
    "    print(f\"✅ Loaded ICU cohort: {len(cohort_df)} hospitalizations\")\n",
    "    print(f\"Mortality rate: {cohort_df['disposition'].mean():.3f}\")\n",
    "    print(f\"Time range: {cohort_df['start_dttm'].min()} to {cohort_df['start_dttm'].max()}\")\n",
    "    \n",
    "else:\n",
    "    raise FileNotFoundError(f\"Cohort file not found at {cohort_path}. Please run 01_cohort.ipynb first.\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample cohort records:\")\n",
    "cohort_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring feature extraction...\n",
      "Using full dataset: 54509 hospitalizations\n",
      "\n",
      "Feature extraction configuration:\n",
      "  vitals: 7 categories\n",
      "  labs: 52 categories\n",
      "  medication_admin_continuous: 19 categories\n",
      "  respiratory_support: 3 categories\n",
      "\n",
      "Extracting features for 54509 hospitalizations\n"
     ]
    }
   ],
   "source": [
    "# Define feature extraction configuration\n",
    "print(\"Configuring feature extraction...\")\n",
    "\n",
    "# OPTION: Set to True for development/testing with smaller dataset\n",
    "USE_SAMPLE_DATA = False  # Set to True to use sample for faster processing\n",
    "SAMPLE_SIZE = 1000  # Number of hospitalizations to sample\n",
    "\n",
    "# Get hospitalization IDs from cohort\n",
    "if USE_SAMPLE_DATA:\n",
    "    print(f\"⚠️ Using sample data with {SAMPLE_SIZE} hospitalizations for testing\")\n",
    "    cohort_sample = cohort_df.sample(n=min(SAMPLE_SIZE, len(cohort_df)), random_state=42)\n",
    "    cohort_ids = cohort_sample['hospitalization_id'].astype(str).unique().tolist()\n",
    "    print(f\"Sampled {len(cohort_ids)} hospitalizations from {len(cohort_df)} total\")\n",
    "else:\n",
    "    cohort_ids = cohort_df['hospitalization_id'].astype(str).unique().tolist()\n",
    "    print(f\"Using full dataset: {len(cohort_ids)} hospitalizations\")\n",
    "\n",
    "# Define category filters for each table\n",
    "category_filters = {\n",
    "    'vitals': [  # Common vital signs\n",
    "        'heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c',\n",
    "        'weight_kg', 'height_cm'\n",
    "    ],\n",
    "    'labs': [  # Common lab values\n",
    "        \"albumin\", \"alkaline_phosphatase\", \"alt\", \"ast\", \"basophils_percent\", \"basophils_absolute\", \n",
    "        \"bicarbonate\", \"bilirubin_total\", \"bilirubin_conjugated\", \"bilirubin_unconjugated\",\n",
    "        \"bun\", \"calcium_total\", \"calcium_ionized\", \"chloride\", \"creatinine\", \"crp\", \n",
    "        \"eosinophils_percent\", \"eosinophils_absolute\", \"esr\", \"ferritin\", \"glucose_f≠ingerstick\", \n",
    "        \"glucose_serum\", \"hemoglobin\", \"phosphate\", \"inr\", \"lactate\", \"ldh\",\n",
    "        \"lymphocytes_percent\", \"lymphocytes_absolute\", \"magnesium\", \"monocytes_percent\", \n",
    "        \"monocytes_absolute\", \"neutrophils_percent\", \"neutrophils_absolute\",\n",
    "        \"pco2_arterial\", \"po2_arterial\", \"pco2_venous\", \"ph_arterial\", \"ph_venous\", \n",
    "        \"platelet_count\", \"potassium\", \"procalcitonin\", \"pt\", \"ptt\", \n",
    "        \"so2_arterial\", \"so2_mixed_venous\", \"so2_central_venous\", \"sodium\",\n",
    "        \"total_protein\", \"troponin_i\", \"troponin_t\", \"wbc\"\n",
    "    ],\n",
    "    'medication_admin_continuous': [  # Vasoactives and sedatives\n",
    "        \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"angiotensin\", \"vasopressin\",\n",
    "        \"dopamine\", \"dobutamine\", \"milrinone\", \"isoproterenol\",\n",
    "        \"propofol\", \"dexmedetomidine\", \"ketamine\", \"midazolam\", \"fentanyl\",\n",
    "        \"hydromorphone\", \"morphine\", \"remifentanil\", \"pentobarbital\", \"lorazepam\"\n",
    "    ],\n",
    "    'respiratory_support': [  # All respiratory support categories\n",
    "        'mode_category', 'device_category', 'fio2'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nFeature extraction configuration:\")\n",
    "for table, categories in category_filters.items():\n",
    "    print(f\"  {table}: {len(categories)} categories\")\n",
    "\n",
    "print(f\"\\nExtracting features for {len(cohort_ids)} hospitalizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Wide Dataset Using pyCLIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Optimization with Cohort Time Filtering\n",
    "\n",
    "The `create_wide_dataset` function now supports an optional `cohort_df` parameter that allows filtering data to specific time windows **before** creating the wide dataset. This significantly improves performance and reduces memory usage when you only need data from specific time periods.\n",
    "\n",
    "**Benefits:**\n",
    "- Reduces data volume before pivoting operations\n",
    "- Significantly lower memory usage\n",
    "- Faster processing time\n",
    "- Particularly useful for ICU mortality models where we only need the first 24 hours\n",
    "\n",
    "**Required columns in cohort_df:**\n",
    "- `hospitalization_id`: Unique identifier for each hospitalization\n",
    "- `start_time`: Start of the time window (datetime)\n",
    "- `end_time`: End of the time window (datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating wide dataset using pyCLIF...\n",
      "Using cohort_df time filtering for 54509 hospitalizations\n",
      "This will filter data to 24-hour windows before creating the wide dataset\n",
      "Auto-loading required base table: patient\n",
      "Loading clif_patient.parquet\n",
      "Data loaded successfully from clif_patient.parquet\n",
      "Validation completed with 8 error(s). See `errors` attribute.\n",
      "Auto-loading required base table: hospitalization\n",
      "Loading clif_hospitalization.parquet\n",
      "Data loaded successfully from clif_hospitalization.parquet\n",
      "Validation completed with 1 error(s). See `errors` attribute.\n",
      "Auto-loading required base table: adt\n",
      "Loading clif_adt.parquet\n",
      "Data loaded successfully from clif_adt.parquet\n",
      "Validation completed with 3 error(s). See `errors` attribute.\n",
      "Auto-loading table: vitals\n",
      "Loading clif_vitals.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924bc894327e41b2bac09a9c67535f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from clif_vitals.parquet\n",
      "Validation completed with 9 error(s).\n",
      "  - 9 range validation error(s)\n",
      "See `errors` and `range_validation_errors` attributes for details.\n",
      "Auto-loading table: labs\n",
      "Loading clif_labs.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55867b0e0384446e93f6e3ac4af56246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from clif_labs.parquet\n",
      "Validation completed with 25 error(s).\n",
      "  - 8 schema validation error(s)\n",
      "  - 17 reference unit error(s)\n",
      "See `errors` and `unit_validation_errors` attributes for details.\n",
      "Auto-loading table: respiratory_support\n",
      "Loading clif_respiratory_support.parquet\n",
      "Data loaded successfully from clif_respiratory_support.parquet\n",
      "Validation completed with 2 error(s). See `errors` attribute.\n",
      "Auto-loading table: medication_admin_continuous\n",
      "Loading clif_medication_admin_continuous.parquet\n",
      "Data loaded successfully from clif_medication_admin_continuous.parquet\n",
      "Validation completed with 4 error(s). See `errors` attribute.\n",
      "Starting wide dataset creation...\n",
      "Using cohort_df with time windows for 54509 hospitalizations\n",
      "Filtering to specific hospitalization IDs: 54509 encounters\n",
      "\n",
      "Loading and filtering base tables...\n",
      "Base tables filtered - Hospitalization: 54509, Patient: 364627, ADT: 217179\n",
      "Processing 54509 hospitalizations in batches of 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 1/6 (10000 hospitalizations)\n",
      "\n",
      "=== Processing Tables ===\n",
      "Base cohort created with 10000 records\n",
      "\n",
      "Processing vitals...\n",
      "Loaded 7657754 records from vitals\n",
      "  Time filtering: 7657754 → 1746779 records\n",
      "Filtering vitals categories to: ['heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c', 'weight_kg', 'height_cm']\n",
      "Pivoted vitals: 400969 combo_ids with 7 category columns\n",
      "\n",
      "Processing labs...\n",
      "Loaded 2634760 records from labs\n",
      "  Time filtering: 2634760 → 550776 records\n",
      "Filtering labs categories to: ['albumin', 'alkaline_phosphatase', 'alt', 'ast', 'basophils_percent', 'basophils_absolute', 'bicarbonate', 'bilirubin_total', 'bilirubin_conjugated', 'bilirubin_unconjugated', 'bun', 'calcium_total', 'calcium_ionized', 'chloride', 'creatinine', 'crp', 'eosinophils_percent', 'eosinophils_absolute', 'esr', 'ferritin', 'glucose_f≠ingerstick', 'glucose_serum', 'hemoglobin', 'phosphate', 'inr', 'lactate', 'ldh', 'lymphocytes_percent', 'lymphocytes_absolute', 'magnesium', 'monocytes_percent', 'monocytes_absolute', 'neutrophils_percent', 'neutrophils_absolute', 'pco2_arterial', 'po2_arterial', 'pco2_venous', 'ph_arterial', 'ph_venous', 'platelet_count', 'potassium', 'procalcitonin', 'pt', 'ptt', 'so2_arterial', 'so2_mixed_venous', 'so2_central_venous', 'sodium', 'total_protein', 'troponin_i', 'troponin_t', 'wbc']\n",
      "Pivoted labs: 125925 combo_ids with 45 category columns\n",
      "\n",
      "Processing medication_admin_continuous...\n",
      "Loaded 511152 records from medication_admin_continuous\n",
      "  Time filtering: 511152 → 149317 records\n",
      "Filtering medication_admin_continuous categories to: ['norepinephrine', 'epinephrine', 'phenylephrine', 'angiotensin', 'vasopressin', 'dopamine', 'dobutamine', 'milrinone', 'isoproterenol', 'propofol', 'dexmedetomidine', 'ketamine', 'midazolam', 'fentanyl', 'hydromorphone', 'morphine', 'remifentanil', 'pentobarbital', 'lorazepam']\n",
      "Pivoted medication_admin_continuous: 71658 combo_ids with 16 category columns\n",
      "\n",
      "Processing respiratory_support...\n",
      "Warning: Columns not found in respiratory_support: ['fio2']\n",
      "Filtered respiratory_support to 4 columns: ['hospitalization_id', 'recorded_dttm', 'mode_category', 'device_category']\n",
      "Loaded 299584 records from respiratory_support\n",
      "  Time filtering: 299584 → 67114 records\n",
      "\n",
      "=== Creating wide dataset ===\n",
      "Executing join query...\n",
      "Added missing column: eosinophils_absolute\n",
      "Added missing column: glucose_f≠ingerstick\n",
      "Added missing column: lymphocytes_absolute\n",
      "Added missing column: monocytes_absolute\n",
      "Added missing column: neutrophils_absolute\n",
      "Added missing column: procalcitonin\n",
      "Added missing column: troponin_i\n",
      "Added missing column: isoproterenol\n",
      "Added missing column: remifentanil\n",
      "Added missing column: pentobarbital\n",
      "Added missing column: fio2\n",
      "Wide dataset created: 680742 records with 91 columns\n",
      "Batch 1 completed: 680742 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  17%|█▋        | 1/6 [00:47<03:55, 47.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 2/6 (10000 hospitalizations)\n",
      "\n",
      "=== Processing Tables ===\n",
      "Base cohort created with 10000 records\n",
      "\n",
      "Processing vitals...\n",
      "Loaded 7504295 records from vitals\n",
      "  Time filtering: 7504295 → 1737265 records\n",
      "Filtering vitals categories to: ['heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c', 'weight_kg', 'height_cm']\n",
      "Pivoted vitals: 401585 combo_ids with 7 category columns\n",
      "\n",
      "Processing labs...\n",
      "Loaded 2625159 records from labs\n",
      "  Time filtering: 2625159 → 546137 records\n",
      "Filtering labs categories to: ['albumin', 'alkaline_phosphatase', 'alt', 'ast', 'basophils_percent', 'basophils_absolute', 'bicarbonate', 'bilirubin_total', 'bilirubin_conjugated', 'bilirubin_unconjugated', 'bun', 'calcium_total', 'calcium_ionized', 'chloride', 'creatinine', 'crp', 'eosinophils_percent', 'eosinophils_absolute', 'esr', 'ferritin', 'glucose_f≠ingerstick', 'glucose_serum', 'hemoglobin', 'phosphate', 'inr', 'lactate', 'ldh', 'lymphocytes_percent', 'lymphocytes_absolute', 'magnesium', 'monocytes_percent', 'monocytes_absolute', 'neutrophils_percent', 'neutrophils_absolute', 'pco2_arterial', 'po2_arterial', 'pco2_venous', 'ph_arterial', 'ph_venous', 'platelet_count', 'potassium', 'procalcitonin', 'pt', 'ptt', 'so2_arterial', 'so2_mixed_venous', 'so2_central_venous', 'sodium', 'total_protein', 'troponin_i', 'troponin_t', 'wbc']\n",
      "Pivoted labs: 124670 combo_ids with 45 category columns\n",
      "\n",
      "Processing medication_admin_continuous...\n",
      "Loaded 497884 records from medication_admin_continuous\n",
      "  Time filtering: 497884 → 145751 records\n",
      "Filtering medication_admin_continuous categories to: ['norepinephrine', 'epinephrine', 'phenylephrine', 'angiotensin', 'vasopressin', 'dopamine', 'dobutamine', 'milrinone', 'isoproterenol', 'propofol', 'dexmedetomidine', 'ketamine', 'midazolam', 'fentanyl', 'hydromorphone', 'morphine', 'remifentanil', 'pentobarbital', 'lorazepam']\n",
      "Pivoted medication_admin_continuous: 71158 combo_ids with 17 category columns\n",
      "\n",
      "Processing respiratory_support...\n",
      "Warning: Columns not found in respiratory_support: ['fio2']\n",
      "Filtered respiratory_support to 4 columns: ['hospitalization_id', 'recorded_dttm', 'mode_category', 'device_category']\n",
      "Loaded 297692 records from respiratory_support\n",
      "  Time filtering: 297692 → 67346 records\n",
      "\n",
      "=== Creating wide dataset ===\n",
      "Executing join query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 2/6 [01:08<02:07, 31.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added missing column: eosinophils_absolute\n",
      "Added missing column: glucose_f≠ingerstick\n",
      "Added missing column: lymphocytes_absolute\n",
      "Added missing column: monocytes_absolute\n",
      "Added missing column: neutrophils_absolute\n",
      "Added missing column: procalcitonin\n",
      "Added missing column: troponin_i\n",
      "Added missing column: isoproterenol\n",
      "Added missing column: remifentanil\n",
      "Added missing column: fio2\n",
      "Wide dataset created: 679346 records with 91 columns\n",
      "Batch 2 completed: 679346 records\n",
      "\n",
      "Processing batch 3/6 (10000 hospitalizations)\n",
      "\n",
      "=== Processing Tables ===\n",
      "Base cohort created with 10000 records\n",
      "\n",
      "Processing vitals...\n",
      "Loaded 7515249 records from vitals\n",
      "  Time filtering: 7515249 → 1739151 records\n",
      "Filtering vitals categories to: ['heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c', 'weight_kg', 'height_cm']\n",
      "Pivoted vitals: 399835 combo_ids with 7 category columns\n",
      "\n",
      "Processing labs...\n",
      "Loaded 2599708 records from labs\n",
      "  Time filtering: 2599708 → 549518 records\n",
      "Filtering labs categories to: ['albumin', 'alkaline_phosphatase', 'alt', 'ast', 'basophils_percent', 'basophils_absolute', 'bicarbonate', 'bilirubin_total', 'bilirubin_conjugated', 'bilirubin_unconjugated', 'bun', 'calcium_total', 'calcium_ionized', 'chloride', 'creatinine', 'crp', 'eosinophils_percent', 'eosinophils_absolute', 'esr', 'ferritin', 'glucose_f≠ingerstick', 'glucose_serum', 'hemoglobin', 'phosphate', 'inr', 'lactate', 'ldh', 'lymphocytes_percent', 'lymphocytes_absolute', 'magnesium', 'monocytes_percent', 'monocytes_absolute', 'neutrophils_percent', 'neutrophils_absolute', 'pco2_arterial', 'po2_arterial', 'pco2_venous', 'ph_arterial', 'ph_venous', 'platelet_count', 'potassium', 'procalcitonin', 'pt', 'ptt', 'so2_arterial', 'so2_mixed_venous', 'so2_central_venous', 'sodium', 'total_protein', 'troponin_i', 'troponin_t', 'wbc']\n",
      "Pivoted labs: 125581 combo_ids with 45 category columns\n",
      "\n",
      "Processing medication_admin_continuous...\n",
      "Loaded 492044 records from medication_admin_continuous\n",
      "  Time filtering: 492044 → 146620 records\n",
      "Filtering medication_admin_continuous categories to: ['norepinephrine', 'epinephrine', 'phenylephrine', 'angiotensin', 'vasopressin', 'dopamine', 'dobutamine', 'milrinone', 'isoproterenol', 'propofol', 'dexmedetomidine', 'ketamine', 'midazolam', 'fentanyl', 'hydromorphone', 'morphine', 'remifentanil', 'pentobarbital', 'lorazepam']\n",
      "Pivoted medication_admin_continuous: 70558 combo_ids with 16 category columns\n",
      "\n",
      "Processing respiratory_support...\n",
      "Warning: Columns not found in respiratory_support: ['fio2']\n",
      "Filtered respiratory_support to 4 columns: ['hospitalization_id', 'recorded_dttm', 'mode_category', 'device_category']\n",
      "Loaded 307250 records from respiratory_support\n",
      "  Time filtering: 307250 → 68226 records\n",
      "\n",
      "=== Creating wide dataset ===\n",
      "Executing join query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  50%|█████     | 3/6 [01:28<01:20, 26.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added missing column: eosinophils_absolute\n",
      "Added missing column: glucose_f≠ingerstick\n",
      "Added missing column: lymphocytes_absolute\n",
      "Added missing column: monocytes_absolute\n",
      "Added missing column: neutrophils_absolute\n",
      "Added missing column: procalcitonin\n",
      "Added missing column: troponin_i\n",
      "Added missing column: angiotensin\n",
      "Added missing column: isoproterenol\n",
      "Added missing column: remifentanil\n",
      "Added missing column: fio2\n",
      "Wide dataset created: 678341 records with 91 columns\n",
      "Batch 3 completed: 678341 records\n",
      "\n",
      "Processing batch 4/6 (10000 hospitalizations)\n",
      "\n",
      "=== Processing Tables ===\n",
      "Base cohort created with 10000 records\n",
      "\n",
      "Processing vitals...\n",
      "Loaded 7707424 records from vitals\n",
      "  Time filtering: 7707424 → 1732914 records\n",
      "Filtering vitals categories to: ['heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c', 'weight_kg', 'height_cm']\n",
      "Pivoted vitals: 397802 combo_ids with 7 category columns\n",
      "\n",
      "Processing labs...\n",
      "Loaded 2625007 records from labs\n",
      "  Time filtering: 2625007 → 543365 records\n",
      "Filtering labs categories to: ['albumin', 'alkaline_phosphatase', 'alt', 'ast', 'basophils_percent', 'basophils_absolute', 'bicarbonate', 'bilirubin_total', 'bilirubin_conjugated', 'bilirubin_unconjugated', 'bun', 'calcium_total', 'calcium_ionized', 'chloride', 'creatinine', 'crp', 'eosinophils_percent', 'eosinophils_absolute', 'esr', 'ferritin', 'glucose_f≠ingerstick', 'glucose_serum', 'hemoglobin', 'phosphate', 'inr', 'lactate', 'ldh', 'lymphocytes_percent', 'lymphocytes_absolute', 'magnesium', 'monocytes_percent', 'monocytes_absolute', 'neutrophils_percent', 'neutrophils_absolute', 'pco2_arterial', 'po2_arterial', 'pco2_venous', 'ph_arterial', 'ph_venous', 'platelet_count', 'potassium', 'procalcitonin', 'pt', 'ptt', 'so2_arterial', 'so2_mixed_venous', 'so2_central_venous', 'sodium', 'total_protein', 'troponin_i', 'troponin_t', 'wbc']\n",
      "Pivoted labs: 124821 combo_ids with 45 category columns\n",
      "\n",
      "Processing medication_admin_continuous...\n",
      "Loaded 511287 records from medication_admin_continuous\n",
      "  Time filtering: 511287 → 148430 records\n",
      "Filtering medication_admin_continuous categories to: ['norepinephrine', 'epinephrine', 'phenylephrine', 'angiotensin', 'vasopressin', 'dopamine', 'dobutamine', 'milrinone', 'isoproterenol', 'propofol', 'dexmedetomidine', 'ketamine', 'midazolam', 'fentanyl', 'hydromorphone', 'morphine', 'remifentanil', 'pentobarbital', 'lorazepam']\n",
      "Pivoted medication_admin_continuous: 72855 combo_ids with 16 category columns\n",
      "\n",
      "Processing respiratory_support...\n",
      "Warning: Columns not found in respiratory_support: ['fio2']\n",
      "Filtered respiratory_support to 4 columns: ['hospitalization_id', 'recorded_dttm', 'mode_category', 'device_category']\n",
      "Loaded 313242 records from respiratory_support\n",
      "  Time filtering: 313242 → 67360 records\n",
      "\n",
      "=== Creating wide dataset ===\n",
      "Executing join query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  67%|██████▋   | 4/6 [01:50<00:49, 24.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added missing column: eosinophils_absolute\n",
      "Added missing column: glucose_f≠ingerstick\n",
      "Added missing column: lymphocytes_absolute\n",
      "Added missing column: monocytes_absolute\n",
      "Added missing column: neutrophils_absolute\n",
      "Added missing column: procalcitonin\n",
      "Added missing column: troponin_i\n",
      "Added missing column: isoproterenol\n",
      "Added missing column: remifentanil\n",
      "Added missing column: lorazepam\n",
      "Added missing column: fio2\n",
      "Wide dataset created: 677298 records with 91 columns\n",
      "Batch 4 completed: 677298 records\n",
      "\n",
      "Processing batch 5/6 (10000 hospitalizations)\n",
      "\n",
      "=== Processing Tables ===\n",
      "Base cohort created with 10000 records\n",
      "\n",
      "Processing vitals...\n",
      "Loaded 7382649 records from vitals\n",
      "  Time filtering: 7382649 → 1734866 records\n",
      "Filtering vitals categories to: ['heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c', 'weight_kg', 'height_cm']\n",
      "Pivoted vitals: 402462 combo_ids with 7 category columns\n",
      "\n",
      "Processing labs...\n",
      "Loaded 2559777 records from labs\n",
      "  Time filtering: 2559777 → 537632 records\n",
      "Filtering labs categories to: ['albumin', 'alkaline_phosphatase', 'alt', 'ast', 'basophils_percent', 'basophils_absolute', 'bicarbonate', 'bilirubin_total', 'bilirubin_conjugated', 'bilirubin_unconjugated', 'bun', 'calcium_total', 'calcium_ionized', 'chloride', 'creatinine', 'crp', 'eosinophils_percent', 'eosinophils_absolute', 'esr', 'ferritin', 'glucose_f≠ingerstick', 'glucose_serum', 'hemoglobin', 'phosphate', 'inr', 'lactate', 'ldh', 'lymphocytes_percent', 'lymphocytes_absolute', 'magnesium', 'monocytes_percent', 'monocytes_absolute', 'neutrophils_percent', 'neutrophils_absolute', 'pco2_arterial', 'po2_arterial', 'pco2_venous', 'ph_arterial', 'ph_venous', 'platelet_count', 'potassium', 'procalcitonin', 'pt', 'ptt', 'so2_arterial', 'so2_mixed_venous', 'so2_central_venous', 'sodium', 'total_protein', 'troponin_i', 'troponin_t', 'wbc']\n",
      "Pivoted labs: 122777 combo_ids with 45 category columns\n",
      "\n",
      "Processing medication_admin_continuous...\n",
      "Loaded 465821 records from medication_admin_continuous\n",
      "  Time filtering: 465821 → 140459 records\n",
      "Filtering medication_admin_continuous categories to: ['norepinephrine', 'epinephrine', 'phenylephrine', 'angiotensin', 'vasopressin', 'dopamine', 'dobutamine', 'milrinone', 'isoproterenol', 'propofol', 'dexmedetomidine', 'ketamine', 'midazolam', 'fentanyl', 'hydromorphone', 'morphine', 'remifentanil', 'pentobarbital', 'lorazepam']\n",
      "Pivoted medication_admin_continuous: 68580 combo_ids with 16 category columns\n",
      "\n",
      "Processing respiratory_support...\n",
      "Warning: Columns not found in respiratory_support: ['fio2']\n",
      "Filtered respiratory_support to 4 columns: ['hospitalization_id', 'recorded_dttm', 'mode_category', 'device_category']\n",
      "Loaded 294534 records from respiratory_support\n",
      "  Time filtering: 294534 → 66819 records\n",
      "\n",
      "=== Creating wide dataset ===\n",
      "Executing join query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  83%|████████▎ | 5/6 [02:12<00:23, 23.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added missing column: eosinophils_absolute\n",
      "Added missing column: glucose_f≠ingerstick\n",
      "Added missing column: lymphocytes_absolute\n",
      "Added missing column: monocytes_absolute\n",
      "Added missing column: neutrophils_absolute\n",
      "Added missing column: procalcitonin\n",
      "Added missing column: troponin_i\n",
      "Added missing column: isoproterenol\n",
      "Added missing column: remifentanil\n",
      "Added missing column: lorazepam\n",
      "Added missing column: fio2\n",
      "Wide dataset created: 675040 records with 91 columns\n",
      "Batch 5 completed: 675040 records\n",
      "\n",
      "Processing batch 6/6 (4509 hospitalizations)\n",
      "\n",
      "=== Processing Tables ===\n",
      "Base cohort created with 4509 records\n",
      "\n",
      "Processing vitals...\n",
      "Loaded 3508848 records from vitals\n",
      "  Time filtering: 3508848 → 779939 records\n",
      "Filtering vitals categories to: ['heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c', 'weight_kg', 'height_cm']\n",
      "Pivoted vitals: 179605 combo_ids with 7 category columns\n",
      "\n",
      "Processing labs...\n",
      "Loaded 1198700 records from labs\n",
      "  Time filtering: 1198700 → 244208 records\n",
      "Filtering labs categories to: ['albumin', 'alkaline_phosphatase', 'alt', 'ast', 'basophils_percent', 'basophils_absolute', 'bicarbonate', 'bilirubin_total', 'bilirubin_conjugated', 'bilirubin_unconjugated', 'bun', 'calcium_total', 'calcium_ionized', 'chloride', 'creatinine', 'crp', 'eosinophils_percent', 'eosinophils_absolute', 'esr', 'ferritin', 'glucose_f≠ingerstick', 'glucose_serum', 'hemoglobin', 'phosphate', 'inr', 'lactate', 'ldh', 'lymphocytes_percent', 'lymphocytes_absolute', 'magnesium', 'monocytes_percent', 'monocytes_absolute', 'neutrophils_percent', 'neutrophils_absolute', 'pco2_arterial', 'po2_arterial', 'pco2_venous', 'ph_arterial', 'ph_venous', 'platelet_count', 'potassium', 'procalcitonin', 'pt', 'ptt', 'so2_arterial', 'so2_mixed_venous', 'so2_central_venous', 'sodium', 'total_protein', 'troponin_i', 'troponin_t', 'wbc']\n",
      "Pivoted labs: 55972 combo_ids with 45 category columns\n",
      "\n",
      "Processing medication_admin_continuous...\n",
      "Loaded 240187 records from medication_admin_continuous\n",
      "  Time filtering: 240187 → 65260 records\n",
      "Filtering medication_admin_continuous categories to: ['norepinephrine', 'epinephrine', 'phenylephrine', 'angiotensin', 'vasopressin', 'dopamine', 'dobutamine', 'milrinone', 'isoproterenol', 'propofol', 'dexmedetomidine', 'ketamine', 'midazolam', 'fentanyl', 'hydromorphone', 'morphine', 'remifentanil', 'pentobarbital', 'lorazepam']\n",
      "Pivoted medication_admin_continuous: 31382 combo_ids with 17 category columns\n",
      "\n",
      "Processing respiratory_support...\n",
      "Warning: Columns not found in respiratory_support: ['fio2']\n",
      "Filtered respiratory_support to 4 columns: ['hospitalization_id', 'recorded_dttm', 'mode_category', 'device_category']\n",
      "Loaded 139285 records from respiratory_support\n",
      "  Time filtering: 139285 → 30124 records\n",
      "\n",
      "=== Creating wide dataset ===\n",
      "Executing join query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 6/6 [02:23<00:00, 23.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added missing column: eosinophils_absolute\n",
      "Added missing column: glucose_f≠ingerstick\n",
      "Added missing column: lymphocytes_absolute\n",
      "Added missing column: monocytes_absolute\n",
      "Added missing column: neutrophils_absolute\n",
      "Added missing column: procalcitonin\n",
      "Added missing column: troponin_i\n",
      "Added missing column: isoproterenol\n",
      "Added missing column: remifentanil\n",
      "Added missing column: fio2\n",
      "Wide dataset created: 303809 records with 91 columns\n",
      "Batch 6 completed: 303809 records\n",
      "\n",
      "Combining 6 batch results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset: 3694576 records with 91 columns\n",
      "✅ Wide dataset created successfully\n",
      "Shape: (3694576, 91)\n",
      "Hospitalizations: 54509\n",
      "Date range: 2105-10-04 16:27:12-06:00 to 2214-05-07 12:50:17-06:00\n"
     ]
    }
   ],
   "source": [
    "# Create wide dataset for cohort hospitalizations\n",
    "print(\"Creating wide dataset using pyCLIF...\")\n",
    "\n",
    "# Prepare cohort_df with required columns for time filtering\n",
    "# This will significantly reduce memory usage by filtering data to only the 24-hour windows\n",
    "cohort_time_filter = cohort_df[['hospitalization_id', 'hour_24_start_dttm', 'hour_24_end_dttm']].copy()\n",
    "cohort_time_filter.columns = ['hospitalization_id', 'start_time', 'end_time']  # Rename to match expected columns\n",
    "\n",
    "print(f\"Using cohort_df time filtering for {len(cohort_time_filter)} hospitalizations\")\n",
    "print(f\"This will filter data to 24-hour windows before creating the wide dataset\")\n",
    "\n",
    "wide_df = clif.create_wide_dataset(\n",
    "    hospitalization_ids=cohort_ids,\n",
    "    cohort_df=cohort_time_filter,  # Pass cohort_df for time window filtering\n",
    "    category_filters=category_filters,  \n",
    "    save_to_data_location=False,\n",
    "    batch_size=10000,\n",
    "    memory_limit='6GB',\n",
    "    threads=4,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Wide dataset created successfully\")\n",
    "print(f\"Shape: {wide_df.shape}\")\n",
    "print(f\"Hospitalizations: {wide_df['hospitalization_id'].nunique()}\")\n",
    "print(f\"Date range: {wide_df['event_time'].min()} to {wide_df['event_time'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hospitalization_id', 'patient_id', 'age_at_admission', 'event_time',\n",
       "       'mode_category', 'device_category', 'angiotensin', 'dexmedetomidine',\n",
       "       'dobutamine', 'dopamine', 'epinephrine', 'fentanyl', 'hydromorphone',\n",
       "       'ketamine', 'lorazepam', 'midazolam', 'milrinone', 'morphine',\n",
       "       'norepinephrine', 'phenylephrine', 'propofol', 'vasopressin', 'albumin',\n",
       "       'alkaline_phosphatase', 'alt', 'ast', 'basophils_absolute',\n",
       "       'basophils_percent', 'bicarbonate', 'bilirubin_conjugated',\n",
       "       'bilirubin_total', 'bilirubin_unconjugated', 'bun', 'calcium_ionized',\n",
       "       'calcium_total', 'chloride', 'creatinine', 'crp', 'eosinophils_percent',\n",
       "       'esr', 'ferritin', 'glucose_serum', 'hemoglobin', 'inr', 'lactate',\n",
       "       'ldh', 'lymphocytes_percent', 'magnesium', 'monocytes_percent',\n",
       "       'neutrophils_percent', 'pco2_arterial', 'pco2_venous', 'ph_arterial',\n",
       "       'ph_venous', 'phosphate', 'platelet_count', 'po2_arterial', 'potassium',\n",
       "       'pt', 'ptt', 'so2_arterial', 'so2_central_venous', 'so2_mixed_venous',\n",
       "       'sodium', 'total_protein', 'troponin_t', 'wbc', 'heart_rate',\n",
       "       'height_cm', 'map', 'respiratory_rate', 'spo2', 'temp_c', 'weight_kg',\n",
       "       'hospital_id', 'in_dttm', 'out_dttm', 'location_category', 'day_number',\n",
       "       'hosp_id_day_key', 'eosinophils_absolute', 'glucose_f≠ingerstick',\n",
       "       'lymphocytes_absolute', 'monocytes_absolute', 'neutrophils_absolute',\n",
       "       'procalcitonin', 'troponin_i', 'isoproterenol', 'remifentanil',\n",
       "       'pentobarbital', 'fio2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angiotensin</th>\n",
       "      <th>dexmedetomidine</th>\n",
       "      <th>dobutamine</th>\n",
       "      <th>dopamine</th>\n",
       "      <th>epinephrine</th>\n",
       "      <th>fentanyl</th>\n",
       "      <th>hydromorphone</th>\n",
       "      <th>ketamine</th>\n",
       "      <th>lorazepam</th>\n",
       "      <th>midazolam</th>\n",
       "      <th>milrinone</th>\n",
       "      <th>morphine</th>\n",
       "      <th>norepinephrine</th>\n",
       "      <th>pentobarbital</th>\n",
       "      <th>phenylephrine</th>\n",
       "      <th>propofol</th>\n",
       "      <th>vasopressin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>16110.000000</td>\n",
       "      <td>1959.000000</td>\n",
       "      <td>7394.000000</td>\n",
       "      <td>10732.000000</td>\n",
       "      <td>36743.000000</td>\n",
       "      <td>605.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>17197.000000</td>\n",
       "      <td>1286.000000</td>\n",
       "      <td>561.000000</td>\n",
       "      <td>98877.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>88924.000000</td>\n",
       "      <td>125945.000000</td>\n",
       "      <td>6486.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15.205185</td>\n",
       "      <td>0.593659</td>\n",
       "      <td>4.568852</td>\n",
       "      <td>7.204130</td>\n",
       "      <td>0.106587</td>\n",
       "      <td>121.776761</td>\n",
       "      <td>2.290408</td>\n",
       "      <td>1.483141</td>\n",
       "      <td>1.887190</td>\n",
       "      <td>3.718082</td>\n",
       "      <td>0.439542</td>\n",
       "      <td>6.327015</td>\n",
       "      <td>0.150532</td>\n",
       "      <td>40.639446</td>\n",
       "      <td>0.998636</td>\n",
       "      <td>37.925861</td>\n",
       "      <td>2.072930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.681054</td>\n",
       "      <td>0.599438</td>\n",
       "      <td>3.916653</td>\n",
       "      <td>49.019950</td>\n",
       "      <td>0.510442</td>\n",
       "      <td>1495.878735</td>\n",
       "      <td>2.645807</td>\n",
       "      <td>5.509270</td>\n",
       "      <td>1.420547</td>\n",
       "      <td>38.330868</td>\n",
       "      <td>3.121410</td>\n",
       "      <td>7.344227</td>\n",
       "      <td>0.395288</td>\n",
       "      <td>207.083008</td>\n",
       "      <td>4.179108</td>\n",
       "      <td>512.616750</td>\n",
       "      <td>7.858685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.657216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.027509</td>\n",
       "      <td>0.300595</td>\n",
       "      <td>2.499743</td>\n",
       "      <td>2.506073</td>\n",
       "      <td>0.015016</td>\n",
       "      <td>25.000002</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.152991</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.249975</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.049994</td>\n",
       "      <td>0.339147</td>\n",
       "      <td>0.300030</td>\n",
       "      <td>19.820170</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.024252</td>\n",
       "      <td>0.592681</td>\n",
       "      <td>4.004278</td>\n",
       "      <td>5.003784</td>\n",
       "      <td>0.030084</td>\n",
       "      <td>50.064121</td>\n",
       "      <td>1.500154</td>\n",
       "      <td>0.300311</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.251238</td>\n",
       "      <td>4.001383</td>\n",
       "      <td>0.100018</td>\n",
       "      <td>2.004924</td>\n",
       "      <td>0.599865</td>\n",
       "      <td>30.155683</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>20.021649</td>\n",
       "      <td>0.802063</td>\n",
       "      <td>5.021847</td>\n",
       "      <td>9.971610</td>\n",
       "      <td>0.095480</td>\n",
       "      <td>100.056763</td>\n",
       "      <td>3.090909</td>\n",
       "      <td>0.500701</td>\n",
       "      <td>2.629917</td>\n",
       "      <td>4.000432</td>\n",
       "      <td>0.394384</td>\n",
       "      <td>8.002672</td>\n",
       "      <td>0.200139</td>\n",
       "      <td>3.676606</td>\n",
       "      <td>1.198607</td>\n",
       "      <td>50.020006</td>\n",
       "      <td>2.400814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>83.689354</td>\n",
       "      <td>38.726226</td>\n",
       "      <td>30.218054</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>41.142862</td>\n",
       "      <td>75000.031250</td>\n",
       "      <td>23.999998</td>\n",
       "      <td>92.526512</td>\n",
       "      <td>4.032787</td>\n",
       "      <td>3370.979736</td>\n",
       "      <td>99.206001</td>\n",
       "      <td>82.946678</td>\n",
       "      <td>61.439026</td>\n",
       "      <td>1430.000000</td>\n",
       "      <td>833.333254</td>\n",
       "      <td>135793.853760</td>\n",
       "      <td>428.571411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       angiotensin  dexmedetomidine   dobutamine     dopamine   epinephrine  \\\n",
       "count    55.000000     16110.000000  1959.000000  7394.000000  10732.000000   \n",
       "mean     15.205185         0.593659     4.568852     7.204130      0.106587   \n",
       "std      18.681054         0.599438     3.916653    49.019950      0.510442   \n",
       "min       0.000000         0.000000     0.000000     0.000000      0.000000   \n",
       "25%       0.027509         0.300595     2.499743     2.506073      0.015016   \n",
       "50%      10.024252         0.592681     4.004278     5.003784      0.030084   \n",
       "75%      20.021649         0.802063     5.021847     9.971610      0.095480   \n",
       "max      83.689354        38.726226    30.218054  4000.000000     41.142862   \n",
       "\n",
       "           fentanyl  hydromorphone    ketamine  lorazepam     midazolam  \\\n",
       "count  36743.000000     605.000000  954.000000  32.000000  17197.000000   \n",
       "mean     121.776761       2.290408    1.483141   1.887190      3.718082   \n",
       "std     1495.878735       2.645807    5.509270   1.420547     38.330868   \n",
       "min        0.000000       0.000000    0.000000   0.000000      0.000000   \n",
       "25%       25.000002       0.500000    0.152991   0.875000      1.000000   \n",
       "50%       50.064121       1.500154    0.300311   2.000000      2.000000   \n",
       "75%      100.056763       3.090909    0.500701   2.629917      4.000432   \n",
       "max    75000.031250      23.999998   92.526512   4.032787   3370.979736   \n",
       "\n",
       "         milrinone    morphine  norepinephrine  pentobarbital  phenylephrine  \\\n",
       "count  1286.000000  561.000000    98877.000000      58.000000   88924.000000   \n",
       "mean      0.439542    6.327015        0.150532      40.639446       0.998636   \n",
       "std       3.121410    7.344227        0.395288     207.083008       4.179108   \n",
       "min       0.000000    0.000000        0.000000       0.000000      -1.657216   \n",
       "25%       0.249975    2.000000        0.049994       0.339147       0.300030   \n",
       "50%       0.251238    4.001383        0.100018       2.004924       0.599865   \n",
       "75%       0.394384    8.002672        0.200139       3.676606       1.198607   \n",
       "max      99.206001   82.946678       61.439026    1430.000000     833.333254   \n",
       "\n",
       "            propofol  vasopressin  \n",
       "count  125945.000000  6486.000000  \n",
       "mean       37.925861     2.072930  \n",
       "std       512.616750     7.858685  \n",
       "min         0.000000     0.000000  \n",
       "25%        19.820170     1.200000  \n",
       "50%        30.155683     2.400000  \n",
       "75%        50.020006     2.400814  \n",
       "max    135793.853760   428.571411  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_df[[ 'angiotensin', 'dexmedetomidine',\n",
    "       'dobutamine', 'dopamine', 'epinephrine', 'fentanyl', 'hydromorphone',\n",
    "       'ketamine', 'lorazepam', 'midazolam', 'milrinone', 'morphine',\n",
    "       'norepinephrine', 'pentobarbital', 'phenylephrine', 'propofol',\n",
    "       'vasopressin']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting data sample...\n",
      "Wide dataset shape: (3694576, 91)\n",
      "Memory usage: 3350.6 MB\n",
      "Examining data for hospitalization: 20001770\n",
      "Records for 20001770: 63\n",
      "Time range: 2117-01-25 13:23:00-06:00 to 2117-01-28 16:35:58-06:00\n",
      "Columns with data: 37\n"
     ]
    }
   ],
   "source": [
    "# Safely inspect a subset of the data to avoid memory issues\n",
    "print(\"Inspecting data sample...\")\n",
    "\n",
    "# Check dataset size first\n",
    "print(f\"Wide dataset shape: {wide_df.shape}\")\n",
    "print(f\"Memory usage: {wide_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Sample a specific hospitalization safely\n",
    "sample_hosp_id = wide_df['hospitalization_id'].iloc[0]\n",
    "print(f\"Examining data for hospitalization: {sample_hosp_id}\")\n",
    "\n",
    "try:\n",
    "    # Use query method which is more memory efficient for large datasets\n",
    "    temp = wide_df.query(f\"hospitalization_id == '{sample_hosp_id}'\")\n",
    "    print(f\"Records for {sample_hosp_id}: {len(temp)}\")\n",
    "    \n",
    "    if len(temp) > 0:\n",
    "        print(\"Time range:\", temp['event_time'].min(), \"to\", temp['event_time'].max())\n",
    "        print(\"Columns with data:\", (temp.notna().sum() > 0).sum())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error inspecting data: {str(e)}\")\n",
    "    print(\"Dataset might be too large for this operation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining aggregation configuration...\n",
      "Aggregation configuration:\n",
      "  max: 59 columns\n",
      "    ['heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c']...['troponin_t', 'wbc'] (showing first 5 and last 2)\n",
      "  min: 59 columns\n",
      "    ['heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c']...['troponin_t', 'wbc'] (showing first 5 and last 2)\n",
      "  median: 59 columns\n",
      "    ['heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c']...['troponin_t', 'wbc'] (showing first 5 and last 2)\n",
      "  boolean: 19 columns\n",
      "    ['norepinephrine', 'epinephrine', 'phenylephrine', 'angiotensin', 'vasopressin']...['pentobarbital', 'lorazepam'] (showing first 5 and last 2)\n",
      "  one_hot_encode: 2 columns\n",
      "    ['mode_category', 'device_category']\n",
      "\n",
      "Processing 3,694,576 records to hourly aggregation...\n",
      "Starting optimized hourly aggregation using DuckDB...\n",
      "Input dataset shape: (3694576, 91)\n",
      "Memory limit: 6GB\n",
      "Processing in batches of 10000 hospitalizations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 1/6:   0%|          | 0/6 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch 1/6 (10000 hospitalizations) ---\n",
      "Creating hourly buckets...\n",
      "Calculating nth_hour...\n",
      "Columns not in aggregation_config, defaulting to 'first' with '_c' postfix:\n",
      "  - age_at_admission\n",
      "  - hospital_id\n",
      "  - in_dttm\n",
      "  - out_dttm\n",
      "  - location_category\n",
      "  - hosp_id_day_key\n",
      "  - fio2\n",
      "\n",
      "Processing aggregations by type:\n",
      "- Extracting base columns...\n",
      "- Processing max aggregation...\n",
      "  ✓ max complete (59 columns)\n",
      "- Processing min aggregation...\n",
      "  ✓ min complete (59 columns)\n",
      "- Processing median aggregation...\n",
      "  ✓ median complete (59 columns)\n",
      "- Processing first aggregation...\n",
      "  ✓ first complete (7 columns)\n",
      "- Processing boolean aggregation...\n",
      "  ✓ boolean complete (19 columns)\n",
      "- Processing one_hot_encode aggregation...\n",
      "  ✓ one_hot_encode complete (16 columns)\n",
      "\n",
      "Merging aggregation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 2/6:  17%|█▋        | 1/6 [00:03<00:15,  3.16s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hourly aggregation complete: 265906 hourly records\n",
      "Columns in hourly dataset: 225\n",
      "Batch 1 completed: 265906 records\n",
      "\n",
      "--- Batch 2/6 (10000 hospitalizations) ---\n",
      "Creating hourly buckets...\n",
      "Calculating nth_hour...\n",
      "Columns not in aggregation_config, defaulting to 'first' with '_c' postfix:\n",
      "  - age_at_admission\n",
      "  - hospital_id\n",
      "  - in_dttm\n",
      "  - out_dttm\n",
      "  - location_category\n",
      "  - hosp_id_day_key\n",
      "  - fio2\n",
      "\n",
      "Processing aggregations by type:\n",
      "- Extracting base columns...\n",
      "- Processing max aggregation...\n",
      "  ✓ max complete (59 columns)\n",
      "- Processing min aggregation...\n",
      "  ✓ min complete (59 columns)\n",
      "- Processing median aggregation...\n",
      "  ✓ median complete (59 columns)\n",
      "- Processing first aggregation...\n",
      "  ✓ first complete (7 columns)\n",
      "- Processing boolean aggregation...\n",
      "  ✓ boolean complete (19 columns)\n",
      "- Processing one_hot_encode aggregation...\n",
      "  ✓ one_hot_encode complete (16 columns)\n",
      "\n",
      "Merging aggregation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 3/6:  33%|███▎      | 2/6 [00:06<00:12,  3.24s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hourly aggregation complete: 265793 hourly records\n",
      "Columns in hourly dataset: 225\n",
      "Batch 2 completed: 265793 records\n",
      "\n",
      "--- Batch 3/6 (10000 hospitalizations) ---\n",
      "Creating hourly buckets...\n",
      "Calculating nth_hour...\n",
      "Columns not in aggregation_config, defaulting to 'first' with '_c' postfix:\n",
      "  - age_at_admission\n",
      "  - hospital_id\n",
      "  - in_dttm\n",
      "  - out_dttm\n",
      "  - location_category\n",
      "  - hosp_id_day_key\n",
      "  - fio2\n",
      "\n",
      "Processing aggregations by type:\n",
      "- Extracting base columns...\n",
      "- Processing max aggregation...\n",
      "  ✓ max complete (59 columns)\n",
      "- Processing min aggregation...\n",
      "  ✓ min complete (59 columns)\n",
      "- Processing median aggregation...\n",
      "  ✓ median complete (59 columns)\n",
      "- Processing first aggregation...\n",
      "  ✓ first complete (7 columns)\n",
      "- Processing boolean aggregation...\n",
      "  ✓ boolean complete (19 columns)\n",
      "- Processing one_hot_encode aggregation...\n",
      "  ✓ one_hot_encode complete (16 columns)\n",
      "\n",
      "Merging aggregation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 4/6:  50%|█████     | 3/6 [00:09<00:09,  3.24s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hourly aggregation complete: 265269 hourly records\n",
      "Columns in hourly dataset: 225\n",
      "Batch 3 completed: 265269 records\n",
      "\n",
      "--- Batch 4/6 (10000 hospitalizations) ---\n",
      "Creating hourly buckets...\n",
      "Calculating nth_hour...\n",
      "Columns not in aggregation_config, defaulting to 'first' with '_c' postfix:\n",
      "  - age_at_admission\n",
      "  - hospital_id\n",
      "  - in_dttm\n",
      "  - out_dttm\n",
      "  - location_category\n",
      "  - hosp_id_day_key\n",
      "  - fio2\n",
      "\n",
      "Processing aggregations by type:\n",
      "- Extracting base columns...\n",
      "- Processing max aggregation...\n",
      "  ✓ max complete (59 columns)\n",
      "- Processing min aggregation...\n",
      "  ✓ min complete (59 columns)\n",
      "- Processing median aggregation...\n",
      "  ✓ median complete (59 columns)\n",
      "- Processing first aggregation...\n",
      "  ✓ first complete (7 columns)\n",
      "- Processing boolean aggregation...\n",
      "  ✓ boolean complete (19 columns)\n",
      "- Processing one_hot_encode aggregation...\n",
      "  ✓ one_hot_encode complete (16 columns)\n",
      "\n",
      "Merging aggregation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 5/6:  67%|██████▋   | 4/6 [00:12<00:06,  3.21s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hourly aggregation complete: 265172 hourly records\n",
      "Columns in hourly dataset: 225\n",
      "Batch 4 completed: 265172 records\n",
      "\n",
      "--- Batch 5/6 (10000 hospitalizations) ---\n",
      "Creating hourly buckets...\n",
      "Calculating nth_hour...\n",
      "Columns not in aggregation_config, defaulting to 'first' with '_c' postfix:\n",
      "  - age_at_admission\n",
      "  - hospital_id\n",
      "  - in_dttm\n",
      "  - out_dttm\n",
      "  - location_category\n",
      "  - hosp_id_day_key\n",
      "  - fio2\n",
      "\n",
      "Processing aggregations by type:\n",
      "- Extracting base columns...\n",
      "- Processing max aggregation...\n",
      "  ✓ max complete (59 columns)\n",
      "- Processing min aggregation...\n",
      "  ✓ min complete (59 columns)\n",
      "- Processing median aggregation...\n",
      "  ✓ median complete (59 columns)\n",
      "- Processing first aggregation...\n",
      "  ✓ first complete (7 columns)\n",
      "- Processing boolean aggregation...\n",
      "  ✓ boolean complete (19 columns)\n",
      "- Processing one_hot_encode aggregation...\n",
      "  ✓ one_hot_encode complete (15 columns)\n",
      "\n",
      "Merging aggregation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 6/6:  83%|████████▎ | 5/6 [00:15<00:03,  3.18s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hourly aggregation complete: 265954 hourly records\n",
      "Columns in hourly dataset: 224\n",
      "Batch 5 completed: 265954 records\n",
      "\n",
      "--- Batch 6/6 (4509 hospitalizations) ---\n",
      "Creating hourly buckets...\n",
      "Calculating nth_hour...\n",
      "Columns not in aggregation_config, defaulting to 'first' with '_c' postfix:\n",
      "  - age_at_admission\n",
      "  - hospital_id\n",
      "  - in_dttm\n",
      "  - out_dttm\n",
      "  - location_category\n",
      "  - hosp_id_day_key\n",
      "  - fio2\n",
      "\n",
      "Processing aggregations by type:\n",
      "- Extracting base columns...\n",
      "- Processing max aggregation...\n",
      "  ✓ max complete (59 columns)\n",
      "- Processing min aggregation...\n",
      "  ✓ min complete (59 columns)\n",
      "- Processing median aggregation...\n",
      "  ✓ median complete (59 columns)\n",
      "- Processing first aggregation...\n",
      "  ✓ first complete (7 columns)\n",
      "- Processing boolean aggregation...\n",
      "  ✓ boolean complete (19 columns)\n",
      "- Processing one_hot_encode aggregation...\n",
      "  ✓ one_hot_encode complete (15 columns)\n",
      "\n",
      "Merging aggregation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 6/6: 100%|██████████| 6/6 [00:17<00:00,  2.93s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hourly aggregation complete: 119783 hourly records\n",
      "Columns in hourly dataset: 224\n",
      "Batch 6 completed: 119783 records\n",
      "\n",
      "Combining 6 batch results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final hourly dataset: 1447877 records from 6 batches\n",
      "✅ Hourly aggregation completed!\n"
     ]
    }
   ],
   "source": [
    "# Define aggregation configuration - FIXED to match available columns\n",
    "print(\"Defining aggregation configuration...\")\n",
    "\n",
    "# Build aggregation config based on what we actually have\n",
    "aggregation_config = {\n",
    "    # Apply multiple aggregations to vital signs and labs that are actually present\n",
    "    \"max\": [col for col in category_filters['vitals'] + category_filters['labs'] if col in wide_df.columns],\n",
    "    \"min\": [col for col in category_filters['vitals'] + category_filters['labs'] if col in wide_df.columns],\n",
    "   # \"mean\": [col for col in category_filters['vitals'] + category_filters['labs'] if col in wide_df.columns],\n",
    "    \"median\": [col for col in category_filters['vitals'] + category_filters['labs'] if col in wide_df.columns],\n",
    "    # Boolean aggregation for medications (1 if present in hour, 0 otherwise)\n",
    "    \"boolean\": [col for col in category_filters['medication_admin_continuous'] if col in wide_df.columns],\n",
    "    # One-hot encode categorical respiratory support columns\n",
    "    \"one_hot_encode\": [col for col in [\"mode_category\", \"device_category\"] if col in wide_df.columns]\n",
    "}\n",
    "\n",
    "# Print what will actually be aggregated\n",
    "print(\"Aggregation configuration:\")\n",
    "for method, cols in aggregation_config.items():\n",
    "    print(f\"  {method}: {len(cols)} columns\")\n",
    "    if len(cols) <= 10:\n",
    "        print(f\"    {cols}\")\n",
    "    else:\n",
    "        print(f\"    {cols[:5]}...{cols[-2:]} (showing first 5 and last 2)\")\n",
    "\n",
    "# Convert to hourly using optimized DuckDB function\n",
    "print(f\"\\nProcessing {len(wide_df):,} records to hourly aggregation...\")\n",
    "\n",
    "hourly_df = convert_wide_to_hourly(\n",
    "    wide_df, \n",
    "    aggregation_config, \n",
    "    memory_limit='6GB',      # Set memory limit for DuckDB\n",
    "    batch_size=10000          # Process in batches for large datasets\n",
    ")\n",
    "\n",
    "print(\"✅ Hourly aggregation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hourly Aggregation Results ===\n",
      "✅ Processing complete!\n",
      "Input wide dataset: 3,694,576 records\n",
      "Output hourly dataset: 1,447,877 records\n",
      "Columns in hourly dataset: 225\n",
      "Compression ratio: 2.6x fewer records\n",
      "\n",
      "Hourly record distribution:\n",
      "  Hours covered: 0 to 9244\n",
      "  Average records per hour: 857\n",
      "  Records in first 24 hours: 915,722\n",
      "\n",
      "Sample of aggregated columns:\n",
      "  heart_rate_max: 1,231,954 non-null values\n",
      "  map_max: 1,211,859 non-null values\n",
      "  respiratory_rate_max: 1,225,913 non-null values\n",
      "  spo2_max: 1,217,815 non-null values\n",
      "  temp_c_max: 387,162 non-null values\n",
      "  weight_kg_max: 74,576 non-null values\n",
      "  height_cm_max: 935 non-null values\n",
      "  albumin_max: 20,848 non-null values\n",
      "  alkaline_phosphatase_max: 36,736 non-null values\n",
      "  alt_max: 37,372 non-null values\n"
     ]
    }
   ],
   "source": [
    "# Performance and Results Summary\n",
    "print(\"=== Hourly Aggregation Results ===\")\n",
    "print(f\"✅ Processing complete!\")\n",
    "print(f\"Input wide dataset: {wide_df.shape[0]:,} records\")\n",
    "print(f\"Output hourly dataset: {hourly_df.shape[0]:,} records\") \n",
    "print(f\"Columns in hourly dataset: {hourly_df.shape[1]}\")\n",
    "print(f\"Compression ratio: {wide_df.shape[0] / hourly_df.shape[0]:.1f}x fewer records\")\n",
    "\n",
    "# Show hourly distribution\n",
    "hourly_stats = hourly_df.groupby('nth_hour').size()\n",
    "print(f\"\\nHourly record distribution:\")\n",
    "print(f\"  Hours covered: 0 to {hourly_stats.index.max()}\")\n",
    "print(f\"  Average records per hour: {hourly_stats.mean():.0f}\")\n",
    "print(f\"  Records in first 24 hours: {hourly_stats[hourly_stats.index < 24].sum():,}\")\n",
    "\n",
    "# Show sample of output columns\n",
    "print(f\"\\nSample of aggregated columns:\")\n",
    "agg_columns = [col for col in hourly_df.columns if any(col.endswith(suffix) for suffix in ['_max', '_min', '_mean', '_boolean'])]\n",
    "for col in agg_columns[:10]:\n",
    "    non_null_count = hourly_df[col].notna().sum()\n",
    "    print(f\"  {col}: {non_null_count:,} non-null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering to 24-hour windows for event wide data...: Shape: (3694576, 91)\n",
      "After merge with cohort: 3694576 records\n",
      "✅ Filtered to 24-hour windows: 3694576 records\n",
      "Hospitalizations with data: 54509\n",
      "\n",
      "Time window validation:\n",
      "All events within window: False\n",
      "Average records per hospitalization: 67.8\n",
      "Shape: after filtering: (3694576, 94)\n"
     ]
    }
   ],
   "source": [
    "# Note: This filtering step is now redundant if cohort_df was used in create_wide_dataset\n",
    "# The data is already filtered to the 24-hour windows during the wide dataset creation\n",
    "# However, we'll keep this for backward compatibility and verification\n",
    "\n",
    "# Filter wide dataset to 24-hour windows\n",
    "print(\"Filtering to 24-hour windows for event wide data...: Shape:\", wide_df.shape)\n",
    "cohort_df['hospitalization_id'] = cohort_df['hospitalization_id'].astype(str)\n",
    "# Merge with cohort to get time windows\n",
    "wide_df_filtered = pd.merge(\n",
    "    wide_df,\n",
    "    cohort_df[['hospitalization_id', 'hour_24_start_dttm', 'hour_24_end_dttm', 'disposition']],\n",
    "    on='hospitalization_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"After merge with cohort: {len(wide_df_filtered)} records\")\n",
    "\n",
    "print(f\"✅ Filtered to 24-hour windows: {len(wide_df_filtered)} records\")\n",
    "print(f\"Hospitalizations with data: {wide_df_filtered['hospitalization_id'].nunique()}\")\n",
    "\n",
    "# Show time window validation\n",
    "print(\"\\nTime window validation:\")\n",
    "print(f\"All events within window: {((wide_df_filtered['event_time'] >= wide_df_filtered['hour_24_start_dttm']) & (wide_df_filtered['event_time'] <= wide_df_filtered['hour_24_end_dttm'])).all()}\")\n",
    "print(f\"Average records per hospitalization: {len(wide_df_filtered) / wide_df_filtered['hospitalization_id'].nunique():.1f}\")\n",
    "print('Shape: after filtering:', wide_df_filtered.shape)\n",
    "\n",
    "wide_df_filtered.to_parquet(os.path.join(output_dir, 'by_event_wide_df.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering hourly dataset to 24-hour windows...| Shape: (1447877, 225)\n",
      "After merge with cohort: 1447877 records\n",
      "✅ Filtered hourly dataset to 24-hour windows: 1447877 records\n",
      "Hospitalizations with data in hourly dataset: 54509\n",
      "\n",
      "Time window validation for hourly dataset:\n",
      "All events within window: False\n",
      "Average records per hospitalization: 26.6\n",
      "Shape: (1447877, 228)\n"
     ]
    }
   ],
   "source": [
    "# Filter hourly dataset to 24-hour windows\n",
    "print(\"\\nFiltering hourly dataset to 24-hour windows...| Shape:\",hourly_df.shape)\n",
    "# Merge with cohort to get time windows\n",
    "hourly_df_filtered = pd.merge(\n",
    "    hourly_df,\n",
    "    cohort_df[['hospitalization_id', 'hour_24_start_dttm', 'hour_24_end_dttm', 'disposition']],\n",
    "    on='hospitalization_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"After merge with cohort: {len(hourly_df_filtered)} records\")\n",
    "\n",
    "print(f\"✅ Filtered hourly dataset to 24-hour windows: {len(hourly_df_filtered)} records\")\n",
    "print(f\"Hospitalizations with data in hourly dataset: {hourly_df_filtered['hospitalization_id'].nunique()}\")\n",
    "\n",
    "# Show time window validation for hourly dataset\n",
    "print(\"\\nTime window validation for hourly dataset:\")\n",
    "print(f\"All events within window: {((hourly_df_filtered['event_time_hour'] >= hourly_df_filtered['hour_24_start_dttm']) & (hourly_df_filtered['event_time_hour'] <= hourly_df_filtered['hour_24_end_dttm'])).all()}\")\n",
    "print(f\"Average records per hospitalization: {len(hourly_df_filtered) / hourly_df_filtered['hospitalization_id'].nunique():.1f}\")\n",
    "\n",
    "print('Shape:', hourly_df_filtered.shape)\n",
    "hourly_df_filtered.to_parquet(os.path.join(output_dir, 'by_hourly_wide_df.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hospitalization_id',\n",
       " 'event_time_hour',\n",
       " 'nth_hour',\n",
       " 'hour_bucket',\n",
       " 'patient_id',\n",
       " 'day_number',\n",
       " 'heart_rate_max',\n",
       " 'map_max',\n",
       " 'respiratory_rate_max',\n",
       " 'spo2_max',\n",
       " 'temp_c_max',\n",
       " 'weight_kg_max',\n",
       " 'height_cm_max',\n",
       " 'albumin_max',\n",
       " 'alkaline_phosphatase_max',\n",
       " 'alt_max',\n",
       " 'ast_max',\n",
       " 'basophils_percent_max',\n",
       " 'basophils_absolute_max',\n",
       " 'bicarbonate_max',\n",
       " 'bilirubin_total_max',\n",
       " 'bilirubin_conjugated_max',\n",
       " 'bilirubin_unconjugated_max',\n",
       " 'bun_max',\n",
       " 'calcium_total_max',\n",
       " 'calcium_ionized_max',\n",
       " 'chloride_max',\n",
       " 'creatinine_max',\n",
       " 'crp_max',\n",
       " 'eosinophils_percent_max',\n",
       " 'eosinophils_absolute_max',\n",
       " 'esr_max',\n",
       " 'ferritin_max',\n",
       " 'glucose_f≠ingerstick_max',\n",
       " 'glucose_serum_max',\n",
       " 'hemoglobin_max',\n",
       " 'phosphate_max',\n",
       " 'inr_max',\n",
       " 'lactate_max',\n",
       " 'ldh_max',\n",
       " 'lymphocytes_percent_max',\n",
       " 'lymphocytes_absolute_max',\n",
       " 'magnesium_max',\n",
       " 'monocytes_percent_max',\n",
       " 'monocytes_absolute_max',\n",
       " 'neutrophils_percent_max',\n",
       " 'neutrophils_absolute_max',\n",
       " 'pco2_arterial_max',\n",
       " 'po2_arterial_max',\n",
       " 'pco2_venous_max',\n",
       " 'ph_arterial_max',\n",
       " 'ph_venous_max',\n",
       " 'platelet_count_max',\n",
       " 'potassium_max',\n",
       " 'procalcitonin_max',\n",
       " 'pt_max',\n",
       " 'ptt_max',\n",
       " 'so2_arterial_max',\n",
       " 'so2_mixed_venous_max',\n",
       " 'so2_central_venous_max',\n",
       " 'sodium_max',\n",
       " 'total_protein_max',\n",
       " 'troponin_i_max',\n",
       " 'troponin_t_max',\n",
       " 'wbc_max',\n",
       " 'heart_rate_min',\n",
       " 'map_min',\n",
       " 'respiratory_rate_min',\n",
       " 'spo2_min',\n",
       " 'temp_c_min',\n",
       " 'weight_kg_min',\n",
       " 'height_cm_min',\n",
       " 'albumin_min',\n",
       " 'alkaline_phosphatase_min',\n",
       " 'alt_min',\n",
       " 'ast_min',\n",
       " 'basophils_percent_min',\n",
       " 'basophils_absolute_min',\n",
       " 'bicarbonate_min',\n",
       " 'bilirubin_total_min',\n",
       " 'bilirubin_conjugated_min',\n",
       " 'bilirubin_unconjugated_min',\n",
       " 'bun_min',\n",
       " 'calcium_total_min',\n",
       " 'calcium_ionized_min',\n",
       " 'chloride_min',\n",
       " 'creatinine_min',\n",
       " 'crp_min',\n",
       " 'eosinophils_percent_min',\n",
       " 'eosinophils_absolute_min',\n",
       " 'esr_min',\n",
       " 'ferritin_min',\n",
       " 'glucose_f≠ingerstick_min',\n",
       " 'glucose_serum_min',\n",
       " 'hemoglobin_min',\n",
       " 'phosphate_min',\n",
       " 'inr_min',\n",
       " 'lactate_min',\n",
       " 'ldh_min',\n",
       " 'lymphocytes_percent_min',\n",
       " 'lymphocytes_absolute_min',\n",
       " 'magnesium_min',\n",
       " 'monocytes_percent_min',\n",
       " 'monocytes_absolute_min',\n",
       " 'neutrophils_percent_min',\n",
       " 'neutrophils_absolute_min',\n",
       " 'pco2_arterial_min',\n",
       " 'po2_arterial_min',\n",
       " 'pco2_venous_min',\n",
       " 'ph_arterial_min',\n",
       " 'ph_venous_min',\n",
       " 'platelet_count_min',\n",
       " 'potassium_min',\n",
       " 'procalcitonin_min',\n",
       " 'pt_min',\n",
       " 'ptt_min',\n",
       " 'so2_arterial_min',\n",
       " 'so2_mixed_venous_min',\n",
       " 'so2_central_venous_min',\n",
       " 'sodium_min',\n",
       " 'total_protein_min',\n",
       " 'troponin_i_min',\n",
       " 'troponin_t_min',\n",
       " 'wbc_min',\n",
       " 'heart_rate_median',\n",
       " 'map_median',\n",
       " 'respiratory_rate_median',\n",
       " 'spo2_median',\n",
       " 'temp_c_median',\n",
       " 'weight_kg_median',\n",
       " 'height_cm_median',\n",
       " 'albumin_median',\n",
       " 'alkaline_phosphatase_median',\n",
       " 'alt_median',\n",
       " 'ast_median',\n",
       " 'basophils_percent_median',\n",
       " 'basophils_absolute_median',\n",
       " 'bicarbonate_median',\n",
       " 'bilirubin_total_median',\n",
       " 'bilirubin_conjugated_median',\n",
       " 'bilirubin_unconjugated_median',\n",
       " 'bun_median',\n",
       " 'calcium_total_median',\n",
       " 'calcium_ionized_median',\n",
       " 'chloride_median',\n",
       " 'creatinine_median',\n",
       " 'crp_median',\n",
       " 'eosinophils_percent_median',\n",
       " 'eosinophils_absolute_median',\n",
       " 'esr_median',\n",
       " 'ferritin_median',\n",
       " 'glucose_f≠ingerstick_median',\n",
       " 'glucose_serum_median',\n",
       " 'hemoglobin_median',\n",
       " 'phosphate_median',\n",
       " 'inr_median',\n",
       " 'lactate_median',\n",
       " 'ldh_median',\n",
       " 'lymphocytes_percent_median',\n",
       " 'lymphocytes_absolute_median',\n",
       " 'magnesium_median',\n",
       " 'monocytes_percent_median',\n",
       " 'monocytes_absolute_median',\n",
       " 'neutrophils_percent_median',\n",
       " 'neutrophils_absolute_median',\n",
       " 'pco2_arterial_median',\n",
       " 'po2_arterial_median',\n",
       " 'pco2_venous_median',\n",
       " 'ph_arterial_median',\n",
       " 'ph_venous_median',\n",
       " 'platelet_count_median',\n",
       " 'potassium_median',\n",
       " 'procalcitonin_median',\n",
       " 'pt_median',\n",
       " 'ptt_median',\n",
       " 'so2_arterial_median',\n",
       " 'so2_mixed_venous_median',\n",
       " 'so2_central_venous_median',\n",
       " 'sodium_median',\n",
       " 'total_protein_median',\n",
       " 'troponin_i_median',\n",
       " 'troponin_t_median',\n",
       " 'wbc_median',\n",
       " 'age_at_admission_c',\n",
       " 'hospital_id_c',\n",
       " 'in_dttm_c',\n",
       " 'out_dttm_c',\n",
       " 'location_category_c',\n",
       " 'hosp_id_day_key_c',\n",
       " 'fio2_c',\n",
       " 'norepinephrine_boolean',\n",
       " 'epinephrine_boolean',\n",
       " 'phenylephrine_boolean',\n",
       " 'angiotensin_boolean',\n",
       " 'vasopressin_boolean',\n",
       " 'dopamine_boolean',\n",
       " 'dobutamine_boolean',\n",
       " 'milrinone_boolean',\n",
       " 'isoproterenol_boolean',\n",
       " 'propofol_boolean',\n",
       " 'dexmedetomidine_boolean',\n",
       " 'ketamine_boolean',\n",
       " 'midazolam_boolean',\n",
       " 'fentanyl_boolean',\n",
       " 'hydromorphone_boolean',\n",
       " 'morphine_boolean',\n",
       " 'remifentanil_boolean',\n",
       " 'pentobarbital_boolean',\n",
       " 'lorazepam_boolean',\n",
       " 'mode_category_Assist_Control_Volume_Control',\n",
       " 'mode_category_Blow_by',\n",
       " 'mode_category_Other',\n",
       " 'mode_category_Pressure_Control',\n",
       " 'mode_category_Pressure_Support_CPAP',\n",
       " 'mode_category_Pressure_Regulated_Volume_Control',\n",
       " 'mode_category_SIMV',\n",
       " 'mode_category_Volume_Support',\n",
       " 'device_category_CPAP',\n",
       " 'device_category_Face_Mask',\n",
       " 'device_category_High_Flow_NC',\n",
       " 'device_category_IMV',\n",
       " 'device_category_NIPPV',\n",
       " 'device_category_Nasal_Cannula',\n",
       " 'device_category_Other',\n",
       " 'device_category_Trach_Collar',\n",
       " 'hour_24_start_dttm',\n",
       " 'hour_24_end_dttm',\n",
       " 'disposition']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_df_filtered.columns.tolist()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flameICU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
