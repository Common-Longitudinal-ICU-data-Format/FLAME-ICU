{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICU Mortality Model - Feature Engineering\n",
    "\n",
    "This notebook loads the ICU cohort and creates hourly wide dataset for the first 24 hours of ICU stay.\n",
    "\n",
    "## Objective\n",
    "- Load ICU cohort from 01_cohort.ipynb\n",
    "- Use pyCLIF to extract features from CLIF tables\n",
    "- Create hourly wide dataset for the first 24 hours\n",
    "- Filter to encounters with complete 24-hour data\n",
    "- Save features for modeling\n",
    "\n",
    "## Feature Sources\n",
    "- **Vitals**: All vital_category values\n",
    "- **Labs**: All lab_category values\n",
    "- **Patient Assessments**: GCS_total, RASS\n",
    "- **Respiratory Support**: Mode, FiO2, PEEP, ventilator settings (with one-hot encoding)\n",
    "- **Medications**: All vasoactives and sedatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "**Memory Management Notes:**\n",
    "- This notebook processes a large dataset (54K+ hospitalizations) \n",
    "- If you encounter kernel crashes or memory errors:\n",
    "  1. Set `USE_SAMPLE_DATA=True` in the configuration cell below\n",
    "  2. Increase `memory_limit` parameter in the hourly aggregation function\n",
    "  3. Reduce `batch_size` parameters if needed\n",
    "- The hourly aggregation function uses DuckDB for optimal performance and automatically handles batching for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyclif import CLIF\n",
    "from pyclif.utils.wide_dataset import convert_wide_to_hourly\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Ensure the directory exists\n",
    "output_dir = os.path.join('..', 'protected_outputs', 'preprocessing')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(\"=== ICU Mortality Model - Feature Engineering ===\")\n",
    "print(\"Setting up environment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    \"\"\"Load configuration from config.json or config_demo.json\"\"\"\n",
    "    # Try config.json first, then config_demo.json\n",
    "    config_path = os.path.join(\"config.json\")\n",
    "    if not os.path.exists(config_path):\n",
    "        config_path = os.path.join(\"config_demo.json\")\n",
    "    \n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = json.load(file)\n",
    "        print(f\"✅ Loaded configuration from {os.path.basename(config_path)}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Configuration file not found. Please create config.json or config_demo.json based on the config_template.\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "print(f\"Site: {config['site']}\")\n",
    "print(f\"Data path: {config['clif2_path']}\")\n",
    "print(f\"File type: {config['filetype']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pyCLIF\n",
    "clif = CLIF(\n",
    "    data_dir=config['clif2_path'],\n",
    "    filetype=config['filetype'],\n",
    "    timezone=\"US/Eastern\"\n",
    ")\n",
    "\n",
    "print(\"✅ pyCLIF initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ICU Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ICU cohort from 01_cohort.ipynb\n",
    "cohort_path = os.path.join('..', 'protected_outputs', 'preprocessing', 'icu_cohort.parquet')\n",
    "\n",
    "if os.path.exists(cohort_path):\n",
    "    cohort_df = pd.read_parquet(cohort_path)\n",
    "    \n",
    "    # Convert datetime columns\n",
    "    datetime_cols = ['start_dttm', 'hour_24_start_dttm', 'hour_24_end_dttm']\n",
    "    for col in datetime_cols:\n",
    "        cohort_df[col] = pd.to_datetime(cohort_df[col])\n",
    "    \n",
    "    print(f\"✅ Loaded ICU cohort: {len(cohort_df)} hospitalizations\")\n",
    "    print(f\"Mortality rate: {cohort_df['disposition'].mean():.3f}\")\n",
    "    print(f\"Time range: {cohort_df['start_dttm'].min()} to {cohort_df['start_dttm'].max()}\")\n",
    "    \n",
    "else:\n",
    "    raise FileNotFoundError(f\"Cohort file not found at {cohort_path}. Please run 01_cohort.ipynb first.\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample cohort records:\")\n",
    "cohort_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature extraction configuration\n",
    "print(\"Configuring feature extraction...\")\n",
    "\n",
    "# OPTION: Set to True for development/testing with smaller dataset\n",
    "USE_SAMPLE_DATA = False  # Set to True to use sample for faster processing\n",
    "SAMPLE_SIZE = 1000  # Number of hospitalizations to sample\n",
    "\n",
    "# Get hospitalization IDs from cohort\n",
    "if USE_SAMPLE_DATA:\n",
    "    print(f\"⚠️ Using sample data with {SAMPLE_SIZE} hospitalizations for testing\")\n",
    "    cohort_sample = cohort_df.sample(n=min(SAMPLE_SIZE, len(cohort_df)), random_state=42)\n",
    "    cohort_ids = cohort_sample['hospitalization_id'].astype(str).unique().tolist()\n",
    "    print(f\"Sampled {len(cohort_ids)} hospitalizations from {len(cohort_df)} total\")\n",
    "else:\n",
    "    cohort_ids = cohort_df['hospitalization_id'].astype(str).unique().tolist()\n",
    "    print(f\"Using full dataset: {len(cohort_ids)} hospitalizations\")\n",
    "\n",
    "# Define category filters for each table\n",
    "category_filters = {\n",
    "    'vitals': [  # Common vital signs\n",
    "        'heart_rate', 'map', 'respiratory_rate', 'spo2', 'temp_c',\n",
    "        'weight_kg', 'height_cm'\n",
    "    ],\n",
    "    'labs': [  # Common lab values\n",
    "        \"albumin\", \"alkaline_phosphatase\", \"alt\", \"ast\", \"basophils_percent\", \"basophils_absolute\", \n",
    "        \"bicarbonate\", \"bilirubin_total\", \"bilirubin_conjugated\", \"bilirubin_unconjugated\",\n",
    "        \"bun\", \"calcium_total\", \"calcium_ionized\", \"chloride\", \"creatinine\", \"crp\", \n",
    "        \"eosinophils_percent\", \"eosinophils_absolute\", \"esr\", \"ferritin\", \"glucose_f≠ingerstick\", \n",
    "        \"glucose_serum\", \"hemoglobin\", \"phosphate\", \"inr\", \"lactate\", \"ldh\",\n",
    "        \"lymphocytes_percent\", \"lymphocytes_absolute\", \"magnesium\", \"monocytes_percent\", \n",
    "        \"monocytes_absolute\", \"neutrophils_percent\", \"neutrophils_absolute\",\n",
    "        \"pco2_arterial\", \"po2_arterial\", \"pco2_venous\", \"ph_arterial\", \"ph_venous\", \n",
    "        \"platelet_count\", \"potassium\", \"procalcitonin\", \"pt\", \"ptt\", \n",
    "        \"so2_arterial\", \"so2_mixed_venous\", \"so2_central_venous\", \"sodium\",\n",
    "        \"total_protein\", \"troponin_i\", \"troponin_t\", \"wbc\"\n",
    "    ],\n",
    "    'medication_admin_continuous': [  # Vasoactives and sedatives\n",
    "        \"norepinephrine\", \"epinephrine\", \"phenylephrine\", \"angiotensin\", \"vasopressin\",\n",
    "        \"dopamine\", \"dobutamine\", \"milrinone\", \"isoproterenol\",\n",
    "        \"propofol\", \"dexmedetomidine\", \"ketamine\", \"midazolam\", \"fentanyl\",\n",
    "        \"hydromorphone\", \"morphine\", \"remifentanil\", \"pentobarbital\", \"lorazepam\"\n",
    "    ],\n",
    "    'respiratory_support': [  # All respiratory support categories\n",
    "        'mode_category', 'device_category', 'fio2'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nFeature extraction configuration:\")\n",
    "for table, categories in category_filters.items():\n",
    "    print(f\"  {table}: {len(categories)} categories\")\n",
    "\n",
    "print(f\"\\nExtracting features for {len(cohort_ids)} hospitalizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Wide Dataset Using pyCLIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Optimization with Cohort Time Filtering\n",
    "\n",
    "The `create_wide_dataset` function now supports an optional `cohort_df` parameter that allows filtering data to specific time windows **before** creating the wide dataset. This significantly improves performance and reduces memory usage when you only need data from specific time periods.\n",
    "\n",
    "**Benefits:**\n",
    "- Reduces data volume before pivoting operations\n",
    "- Significantly lower memory usage\n",
    "- Faster processing time\n",
    "- Particularly useful for ICU mortality models where we only need the first 24 hours\n",
    "\n",
    "**Required columns in cohort_df:**\n",
    "- `hospitalization_id`: Unique identifier for each hospitalization\n",
    "- `start_time`: Start of the time window (datetime)\n",
    "- `end_time`: End of the time window (datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wide dataset for cohort hospitalizations\n",
    "print(\"Creating wide dataset using pyCLIF...\")\n",
    "\n",
    "# Prepare cohort_df with required columns for time filtering\n",
    "# This will significantly reduce memory usage by filtering data to only the 24-hour windows\n",
    "cohort_time_filter = cohort_df[['hospitalization_id', 'hour_24_start_dttm', 'hour_24_end_dttm']].copy()\n",
    "cohort_time_filter.columns = ['hospitalization_id', 'start_time', 'end_time']  # Rename to match expected columns\n",
    "\n",
    "print(f\"Using cohort_df time filtering for {len(cohort_time_filter)} hospitalizations\")\n",
    "print(f\"This will filter data to 24-hour windows before creating the wide dataset\")\n",
    "\n",
    "wide_df = clif.create_wide_dataset(\n",
    "    hospitalization_ids=cohort_ids,\n",
    "    cohort_df=cohort_time_filter,  # Pass cohort_df for time window filtering\n",
    "    category_filters=category_filters,  \n",
    "    save_to_data_location=False,\n",
    "    batch_size=10000,\n",
    "    memory_limit='6GB',\n",
    "    threads=4,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Wide dataset created successfully\")\n",
    "print(f\"Shape: {wide_df.shape}\")\n",
    "print(f\"Hospitalizations: {wide_df['hospitalization_id'].nunique()}\")\n",
    "print(f\"Date range: {wide_df['event_time'].min()} to {wide_df['event_time'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df[[ 'angiotensin', 'dexmedetomidine',\n",
    "       'dobutamine', 'dopamine', 'epinephrine', 'fentanyl', 'hydromorphone',\n",
    "       'ketamine', 'lorazepam', 'midazolam', 'milrinone', 'morphine',\n",
    "       'norepinephrine', 'pentobarbital', 'phenylephrine', 'propofol',\n",
    "       'vasopressin']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safely inspect a subset of the data to avoid memory issues\n",
    "print(\"Inspecting data sample...\")\n",
    "\n",
    "# Check dataset size first\n",
    "print(f\"Wide dataset shape: {wide_df.shape}\")\n",
    "print(f\"Memory usage: {wide_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Sample a specific hospitalization safely\n",
    "sample_hosp_id = wide_df['hospitalization_id'].iloc[0]\n",
    "print(f\"Examining data for hospitalization: {sample_hosp_id}\")\n",
    "\n",
    "try:\n",
    "    # Use query method which is more memory efficient for large datasets\n",
    "    temp = wide_df.query(f\"hospitalization_id == '{sample_hosp_id}'\")\n",
    "    print(f\"Records for {sample_hosp_id}: {len(temp)}\")\n",
    "    \n",
    "    if len(temp) > 0:\n",
    "        print(\"Time range:\", temp['event_time'].min(), \"to\", temp['event_time'].max())\n",
    "        print(\"Columns with data:\", (temp.notna().sum() > 0).sum())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error inspecting data: {str(e)}\")\n",
    "    print(\"Dataset might be too large for this operation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define aggregation configuration - FIXED to match available columns\n",
    "print(\"Defining aggregation configuration...\")\n",
    "\n",
    "# Build aggregation config based on what we actually have\n",
    "aggregation_config = {\n",
    "    # Apply multiple aggregations to vital signs and labs that are actually present\n",
    "    \"max\": [col for col in category_filters['vitals'] + category_filters['labs'] if col in wide_df.columns],\n",
    "    \"min\": [col for col in category_filters['vitals'] + category_filters['labs'] if col in wide_df.columns],\n",
    "   # \"mean\": [col for col in category_filters['vitals'] + category_filters['labs'] if col in wide_df.columns],\n",
    "    \"median\": [col for col in category_filters['vitals'] + category_filters['labs'] if col in wide_df.columns],\n",
    "    # Boolean aggregation for medications (1 if present in hour, 0 otherwise)\n",
    "    \"boolean\": [col for col in category_filters['medication_admin_continuous'] if col in wide_df.columns],\n",
    "    # One-hot encode categorical respiratory support columns\n",
    "    \"one_hot_encode\": [col for col in [\"mode_category\", \"device_category\"] if col in wide_df.columns]\n",
    "}\n",
    "\n",
    "# Print what will actually be aggregated\n",
    "print(\"Aggregation configuration:\")\n",
    "for method, cols in aggregation_config.items():\n",
    "    print(f\"  {method}: {len(cols)} columns\")\n",
    "    if len(cols) <= 10:\n",
    "        print(f\"    {cols}\")\n",
    "    else:\n",
    "        print(f\"    {cols[:5]}...{cols[-2:]} (showing first 5 and last 2)\")\n",
    "\n",
    "# Convert to hourly using optimized DuckDB function\n",
    "print(f\"\\nProcessing {len(wide_df):,} records to hourly aggregation...\")\n",
    "\n",
    "hourly_df = convert_wide_to_hourly(\n",
    "    wide_df, \n",
    "    aggregation_config, \n",
    "    memory_limit='6GB',      # Set memory limit for DuckDB\n",
    "    batch_size=10000          # Process in batches for large datasets\n",
    ")\n",
    "\n",
    "print(\"✅ Hourly aggregation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance and Results Summary\n",
    "print(\"=== Hourly Aggregation Results ===\")\n",
    "print(f\"✅ Processing complete!\")\n",
    "print(f\"Input wide dataset: {wide_df.shape[0]:,} records\")\n",
    "print(f\"Output hourly dataset: {hourly_df.shape[0]:,} records\") \n",
    "print(f\"Columns in hourly dataset: {hourly_df.shape[1]}\")\n",
    "print(f\"Compression ratio: {wide_df.shape[0] / hourly_df.shape[0]:.1f}x fewer records\")\n",
    "\n",
    "# Show hourly distribution\n",
    "hourly_stats = hourly_df.groupby('nth_hour').size()\n",
    "print(f\"\\nHourly record distribution:\")\n",
    "print(f\"  Hours covered: 0 to {hourly_stats.index.max()}\")\n",
    "print(f\"  Average records per hour: {hourly_stats.mean():.0f}\")\n",
    "print(f\"  Records in first 24 hours: {hourly_stats[hourly_stats.index < 24].sum():,}\")\n",
    "\n",
    "# Show sample of output columns\n",
    "print(f\"\\nSample of aggregated columns:\")\n",
    "agg_columns = [col for col in hourly_df.columns if any(col.endswith(suffix) for suffix in ['_max', '_min', '_mean', '_boolean'])]\n",
    "for col in agg_columns[:10]:\n",
    "    non_null_count = hourly_df[col].notna().sum()\n",
    "    print(f\"  {col}: {non_null_count:,} non-null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This filtering step is now redundant if cohort_df was used in create_wide_dataset\n",
    "# The data is already filtered to the 24-hour windows during the wide dataset creation\n",
    "# However, we'll keep this for backward compatibility and verification\n",
    "\n",
    "# Filter wide dataset to 24-hour windows\n",
    "print(\"Filtering to 24-hour windows for event wide data...: Shape:\", wide_df.shape)\n",
    "cohort_df['hospitalization_id'] = cohort_df['hospitalization_id'].astype(str)\n",
    "# Merge with cohort to get time windows\n",
    "wide_df_filtered = pd.merge(\n",
    "    wide_df,\n",
    "    cohort_df[['hospitalization_id', 'hour_24_start_dttm', 'hour_24_end_dttm', 'disposition']],\n",
    "    on='hospitalization_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"After merge with cohort: {len(wide_df_filtered)} records\")\n",
    "\n",
    "print(f\"✅ Filtered to 24-hour windows: {len(wide_df_filtered)} records\")\n",
    "print(f\"Hospitalizations with data: {wide_df_filtered['hospitalization_id'].nunique()}\")\n",
    "\n",
    "# Show time window validation\n",
    "print(\"\\nTime window validation:\")\n",
    "print(f\"All events within window: {((wide_df_filtered['event_time'] >= wide_df_filtered['hour_24_start_dttm']) & (wide_df_filtered['event_time'] <= wide_df_filtered['hour_24_end_dttm'])).all()}\")\n",
    "print(f\"Average records per hospitalization: {len(wide_df_filtered) / wide_df_filtered['hospitalization_id'].nunique():.1f}\")\n",
    "print('Shape: after filtering:', wide_df_filtered.shape)\n",
    "\n",
    "wide_df_filtered.to_parquet(os.path.join(output_dir, 'by_event_wide_df.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter hourly dataset to 24-hour windows\n",
    "print(\"\\nFiltering hourly dataset to 24-hour windows...| Shape:\",hourly_df.shape)\n",
    "# Merge with cohort to get time windows\n",
    "hourly_df_filtered = pd.merge(\n",
    "    hourly_df,\n",
    "    cohort_df[['hospitalization_id', 'hour_24_start_dttm', 'hour_24_end_dttm', 'disposition']],\n",
    "    on='hospitalization_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"After merge with cohort: {len(hourly_df_filtered)} records\")\n",
    "\n",
    "print(f\"✅ Filtered hourly dataset to 24-hour windows: {len(hourly_df_filtered)} records\")\n",
    "print(f\"Hospitalizations with data in hourly dataset: {hourly_df_filtered['hospitalization_id'].nunique()}\")\n",
    "\n",
    "# Show time window validation for hourly dataset\n",
    "print(\"\\nTime window validation for hourly dataset:\")\n",
    "print(f\"All events within window: {((hourly_df_filtered['event_time_hour'] >= hourly_df_filtered['hour_24_start_dttm']) & (hourly_df_filtered['event_time_hour'] <= hourly_df_filtered['hour_24_end_dttm'])).all()}\")\n",
    "print(f\"Average records per hospitalization: {len(hourly_df_filtered) / hourly_df_filtered['hospitalization_id'].nunique():.1f}\")\n",
    "\n",
    "print('Shape:', hourly_df_filtered.shape)\n",
    "hourly_df_filtered.to_parquet(os.path.join(output_dir, 'by_hourly_wide_df.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df_filtered.columns.tolist()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flameICU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
